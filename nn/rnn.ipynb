{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рекуррентные нейронные сети\n",
    "\n",
    "Рекуррентные нейронные сети (RNN) — вид нейронных сетей, где **связи между элементами образуют направленную последовательность**. Благодаря этому появляется возможность обрабатывать серии событий во времени или последовательные пространственные цепочки. В отличие от многослойных перцептронов, рекуррентные сети могут использовать свою **внутреннюю память** для обработки последовательностей произвольной длины.\n",
    "\n",
    "* [Рекуррентная нейронная сеть - wiki](https://ru.wikipedia.org/wiki/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D0%B0%D1%8F_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C)\n",
    "* [The Unreasonable Effectiveness of Recurrent Neural Networks - Andrej Karpathy blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мотивация к применению\n",
    "\n",
    "Среди основных ограничений DNN и CNN (feedforward networks) можно перечислить следующие:\n",
    "* На вход принимается **вектор фиксированного размера** и на выходе также создается вектор фиксированного размера;\n",
    "* Каждый вход подается в нейронную сеть **независимо**, входные элементы не взаимосвязаны.\n",
    "\n",
    "Но что, если во входные данные представляют из себя что-то целое, но разбитое на части? Текст - последовательность слов, ДНК - последовательность нуклеотидов. Подобные цепочки элементов могут иметь переменную длину.\n",
    "\n",
    "Для решения задач с такими входными данными была придумана архитектура RNN. В RNN входы зависимы: друг от друга, от своих предыдущих **скрытых** состояний. Они как бы \"запоминают\" историю всех приходящих к ним на вход данных. Получается, что на этапе обучения нейрон будет принимать на вход свое же значение, вычисленное на предыдущем по \"времени\" шаге, то есть будет иметь **рекуррентную зависимость**. \n",
    "\n",
    "RNN позволяет работать с **последовательностями** векторов с произвольной длиной, позволяя воплотить разные **архитектурные аспекты**:\n",
    "\n",
    "<img src=\"pictures/rnn_arc.jpeg\" width=600 height=600 />\n",
    "\n",
    "Благодаря этому можно решать, например, такие задачи, как OCR, машинный перевод, генерация текстов, транскрибация текста и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## История развития архитектур\n",
    "\n",
    "| Год | Архитектура | Исследователи | Примечание |\n",
    "| --- | --- | --- | --- |\n",
    "| 1980 | Time-delay neural networks | Хинтон | Свертка по времени |\n",
    "| 1982 | Сеть Хопфилда | Хопфилд |  |\n",
    "| 1986 | Сеть Джордана | Джордан | SRN (Simple recurrent networks) |\n",
    "| 1990 | Сеть Элмана | Элман | SRN (Simple recurrent networks) |\n",
    "| 1997 | [LSTM](https://www.bioinf.jku.at/publications/older/2604.pdf) | Хохрайтер, Шмидхубер |  |\n",
    "| 2014 | [GRU](https://arxiv.org/pdf/1406.1078.pdf) | Чо, ..., Бенжио |  |\n",
    "| 2015 | [MUT1](https://proceedings.mlr.press/v37/jozefowicz15.pdf) | ..., Суцкевер |  |\n",
    "| 2015 | [SCRN (Structurally Constrained Recurrent Neural Network)](https://arxiv.org/pdf/1412.7753v2.pdf) | Миколов | В 4 раза меньше параметров, чем у LSTM |\n",
    "| 2015 | [uRNN (unitary RNN)](https://arxiv.org/pdf/1511.06464.pdf) | ..., Бенжио |  |\n",
    "| 2017 | [SRU (Simple Recurrent Unit)](https://arxiv.org/pdf/1709.02755v5.pdf) |  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура RNN\n",
    "\n",
    "Как вычислять градиент функции, если нейрон принимает на вход свое же значение, вычисленное на предыдущем по \"времени\" шаге? На самом деле, противоречий здесь нет, и в таком графе вычислений не будет никаких циклов. Андрей Карпаты [тонко подмечает](http://karpathy.github.io/2015/05/21/rnn-effectiveness/):\n",
    "\n",
    "> If training vanilla neural nets is optimization over functions, training recurrent nets is optimization over programs.\n",
    "\n",
    "Архитектуру RNN можно представить (разворачивание во времени) как последовательные копии одной и той же ячейки, в которую передается некоторая переменная $h$ - **вектор скрытых состояний**. Мы каждый раз объединяем вектор скрытых состояний с предыдущего шага и входной вектор, получая новый вектор скрытых состояний:\n",
    "\n",
    "<img src=\"pictures/rnn_forward.png\" width=600 height=600 /> \n",
    "<img src=\"pictures/rnn_notation.png\" width=600 height=600 />\n",
    "\n",
    "Vanilla [RNN single cell](https://medium.com/@koushikkushal95/recurrent-neural-networks-part-1-78302d544466) можно представить так, как показано на картинке ниже. Также ниже реализация такой сети на numpy.\n",
    "\n",
    "<img src=\"pictures/rnn_cell.jpg\" width=500 height=500 />\n",
    "\n",
    "В вычислениях участвуют постоянно обновляющиеся матрицы $W_{hh}$, $W_{hx}$ и $W_{hy}$. Получается, что на каждом шаге RNN обучает столько итераций, сколько элементов в последовательности - $t = 1, .., T$. Но веса эти на каждом слое одинаковые (shared weights).\n",
    "\n",
    "<img src=\"pictures/rnn.png\" width=500 height=500 />\n",
    "\n",
    "**RNN Forward propagation:**\n",
    "* $h_t = \\tanh (W_{hh} h_{t-1} + W_{hx} x_{t})$\n",
    "* $\\hat y_t = \\text{softmax} (W_{yh} h_t)$\n",
    "\n",
    "[**RNN Backward propagation:**](https://mmuratarat.github.io/2019-02-07/bptt-of-rnn)\n",
    "* $L(\\hat y, y) = \\sum_{t=1}^T L_t(\\hat y_t,y_t)$ - заданная лосс-функция\n",
    "* Веса матрицы $W_{hh}$ при \"разворачивании\" графа реккурентной нейронной сети будут участвовать в частных производных backpropagation $T-1$ раз. И никаких циклов.\n",
    "\n",
    "Основными проблемами RNN явлвются:\n",
    "* Проблема взрывающегося градиента\n",
    "* Проблема \"влияния\" входа или текущего состояния на ответы сети (не затухание, а, скорее, нераспространение): сеть понимает ближайших соседей по буквам или по словам, но \"далекие\" зависимости, скорее всего, не распознает."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Глубокие RNN\n",
    "\n",
    "Vanila RNN - это сеть, в которой используется всего один уровень нейронов. У такой сети могут быть проблемы с обнаружением сложных зависимостей в данных. Тогда мы можем либо усложнять имеющуюся архитектуру, либо предложить что-то новое.\n",
    "\n",
    "Усложнение архитектуры главным образом связано с моделированием односложных компонент RNN глубокими сетями:\n",
    "* Можно заменить на многосложную конструкцию функцию от входа к скрытому слою\n",
    "* Можно заменить на многосложную конструкцию функцию от скрытого слоя к выходу\n",
    "* Можно заменить на многосложную конструкцию функцию внутри ячейки RNN\n",
    "\n",
    "> Shortcut connections\n",
    "\n",
    "Последний вариант предполагает, что мы рассматриваем реккурентную сеть как слой некоторой другой сети, то есть выходы реккурентного слоя будут входами какого-то другого слоя.\n",
    "\n",
    "### Bidirectional RNN\n",
    "\n",
    "Bidirectional RNN - это реализация RNN, которая пытается решить проблему \"влияния\" входа на результат. По сути, это 2 прогона RNN для входной последовательности: слева направо и справо налево. Так мы получаем 2 независимых матрицы скрытых состояний, отражающих контекст для каждого слова и слева, и справа.\n",
    "\n",
    "<img src=\"pictures/bidirectional_rnn.jpg\" width=500 height=500 />\n",
    "\n",
    "[picture source](https://colah.github.io/posts/2015-09-NN-Types-FP/?ref=blog.paperspace.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN для классификации текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import spacy\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext import datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "spacy.load('en_core_web_sm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "df['sentiment'] = np.where(df['sentiment'] == 'positive', 1, 0)\n",
    "df.columns = ['TEXT_COLUMN_NAME', 'LABEL_COLUMN_NAME']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT_COLUMN_NAME</th>\n",
       "      <th>LABEL_COLUMN_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    TEXT_COLUMN_NAME  LABEL_COLUMN_NAME\n",
       "0  One of the other reviewers has mentioned that ...                  1\n",
       "1  A wonderful little production. <br /><br />The...                  1\n",
       "2  I thought this was a wonderful way to spend ti...                  1\n",
       "3  Basically there's a family where a little boy ...                  0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...                  1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('moviedata.csv', index=None)\n",
    "df = pd.read_csv('moviedata.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка данных при помощи torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kate/anaconda3/lib/python3.8/site-packages/spacy/util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.4.0) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.3.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Define feature processing (torchtext Field)\n",
    "TEXT = torchtext.legacy.data.Field(tokenize='spacy',\n",
    "                                   include_lengths=True,\n",
    "                                   tokenizer_language='en_core_web_sm')\n",
    "LABEL = torchtext.legacy.data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchtext TabularDataset\n",
    "fields = [('TEXT_COLUMN_NAME', TEXT), ('LABEL_COLUMN_NAME', LABEL)]\n",
    "dataset = torchtext.legacy.data.TabularDataset(path='moviedata.csv',\n",
    "                                               format='csv',\n",
    "                                               skip_header=True,\n",
    "                                               fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data 40000\n",
      "Length of test data 10000\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = dataset.split(split_ratio=[0.8, 0.2],\n",
    "                                      random_state=random.seed(RANDOM_SEED))\n",
    "\n",
    "print('Length of train data', len(train_data))\n",
    "print('Length of test data', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data 34000\n",
      "Length of valid data 6000\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = train_data.split(split_ratio=[0.85, 0.15],\n",
    "                                        random_state=random.seed(RANDOM_SEED))\n",
    "\n",
    "print('Length of train data', len(train_data))\n",
    "print('Length of valid data', len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20002\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "# Build Vocabulary\n",
    "VOCABULARY_SIZE = 20000\n",
    "TEXT.build_vocab(train_data,\n",
    "                 max_size=VOCABULARY_SIZE,\n",
    "                 vectors='glove.6B.100d',\n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
    "print(f'Number of classes: {len(LABEL.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataloader\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader, valid_loader, test_loader = torchtext.legacy.data.BucketIterator.splits(\n",
    "    (train_data, val_data, test_data), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.TEXT_COLUMN_NAME),\n",
    "    sort_within_batch=True,\n",
    "    device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация Vanilla RNN\n",
    "\n",
    "[Inspiration code source](https://github.com/hrishikeshshekhar/Vanilla-RNN/tree/master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN:\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 sentence_length, \n",
    "                 hidden_dim=64,\n",
    "                 learning_rate=0.001,\n",
    "                 momentum=0.9):\n",
    "\n",
    "        self.minibatches = 1\n",
    "        self.beta = momentum\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.sentence_length = sentence_length\n",
    "\n",
    "        # Calculating initializer constants (xavier)\n",
    "        whx = math.sqrt(6) / math.sqrt(self.hidden_dim + self.input_dim)\n",
    "        whh = math.sqrt(6) / math.sqrt(self.hidden_dim + self.hidden_dim)\n",
    "        wyh = math.sqrt(6) / math.sqrt(self.output_dim + self.hidden_dim)\n",
    "\n",
    "        # Initial weights\n",
    "        self.Whx = np.random.uniform(-whx, whx, (hidden_dim, input_dim))\n",
    "        self.Whh = np.random.uniform(-whh, whh, (hidden_dim, hidden_dim))\n",
    "        self.Wyh = np.random.uniform(-wyh, wyh, (output_dim, hidden_dim))\n",
    "        \n",
    "        # normal distribution\n",
    "        # self.Whx = np.random.normal(0, 1, (hidden_dim, input_dim))\n",
    "        # self.Whh = np.random.normal(0, 1, (hidden_dim, hidden_dim))\n",
    "        # self.Wyh = np.random.normal(0, 1, (output_dim, hidden_dim))\n",
    "\n",
    "        # Initial biases\n",
    "        self.bh = np.random.normal(0, 1, (hidden_dim, 1))\n",
    "        self.by = np.random.normal(0, 1, (output_dim, 1))\n",
    "\n",
    "        # Momentum optimizer variables\n",
    "        self.dWhx = np.zeros((hidden_dim, input_dim))\n",
    "        self.dWhh = np.zeros((hidden_dim, hidden_dim))\n",
    "        self.dWyh = np.zeros((output_dim, hidden_dim))\n",
    "        self.dbh = np.zeros((hidden_dim, 1))\n",
    "        self.dby  = np.zeros((output_dim, 1))\n",
    "\n",
    "        # Count total trainable parameters\n",
    "        total_params = (\n",
    "            (self.hidden_dim * self.hidden_dim)\n",
    "            + (self.input_dim * self.hidden_dim)\n",
    "            + (self.output_dim * self.hidden_dim)\n",
    "            + (self.hidden_dim)\n",
    "            + (self.output_dim)\n",
    "        )\n",
    "        print(f\"Total trainable parameters: {total_params}\")\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Check input dimensions\n",
    "        assert(data.shape[0] == self.sentence_length)\n",
    "        assert(data.shape[1] == self.input_dim)\n",
    "\n",
    "        # Initializing initial state as a zero vector\n",
    "        batch_size = data.shape[2]\n",
    "        h = np.zeros((self.hidden_dim, batch_size))\n",
    "\n",
    "        # Calculating states\n",
    "        states = [h]\n",
    "        for words in data:\n",
    "            assert(np.array(words).shape == (self.input_dim, batch_size))\n",
    "            h = np.tanh(np.matmul(self.Whh, h) + np.matmul(self.Whx, words) + self.bh)\n",
    "            states.append(h)\n",
    "\n",
    "        # [sentence_length + 1, self.hidden_dim, batch_size]\n",
    "        states = np.array(states)\n",
    "\n",
    "        # Calculating output: [self.output_dim, batch_size]\n",
    "        output = np.matmul(self.Wyh, states[-1]) + self.by\n",
    "        output = np.array(self.softmax(output)).reshape((self.output_dim, batch_size))\n",
    "\n",
    "        return output, states\n",
    "\n",
    "    def softmax(self, data):\n",
    "        np.clip(data, -200, 200, out=data)\n",
    "        data = np.exp(data)\n",
    "        data /= np.sum(data, axis=0)\n",
    "        return data\n",
    "\n",
    "    def backward(self, train_X, train_Y, preds, states, batch_size):\n",
    "        # [self.output_dim, batch_size]\n",
    "        dL_dY = preds.T\n",
    "\n",
    "        for index, pred in enumerate(dL_dY):\n",
    "            pred[train_Y[index]] -= 1\n",
    "\n",
    "        dL_dY = dL_dY.T\n",
    "\n",
    "        # Timesteps\n",
    "        T = len(states) - 1\n",
    "\n",
    "        # Calculating gradients for Wyh and by\n",
    "        # Shape (self.output_dim, self.hidden_dim)\n",
    "        dL_dWyh = np.matmul(dL_dY, states[-1].T)\n",
    "        dL_dby = np.sum(dL_dY, axis=1).reshape(self.output_dim, 1)\n",
    "\n",
    "        # Calculating gradients for Whx, Whh, bh\n",
    "        dL_dWhh = np.zeros(shape=self.Whh.shape)\n",
    "        dL_dWhx = np.zeros(shape=self.Whx.shape)\n",
    "        dL_dbh = np.zeros(shape=self.bh.shape)\n",
    "\n",
    "        # Calculating dL / dh\n",
    "        # Shape(self.hidden_dim, batch_size)\n",
    "        dL_dh = np.matmul(self.Wyh.T, dL_dY)\n",
    "\n",
    "        for t in reversed(range(T)):\n",
    "            dL_dWhh += np.matmul(dL_dh * (1 - (states[t + 1] ** 2)), states[t].T)\n",
    "            dL_dWhx += np.matmul(dL_dh * (1 - (states[t + 1] ** 2)), train_X[t].T)\n",
    "\n",
    "            dL_dbh += np.sum(dL_dh * (1 - (states[t + 1] ** 2)), axis=1).reshape(self.hidden_dim, 1)\n",
    "\n",
    "            # Updating dL_dh\n",
    "            dL_dh = np.matmul(self.Whh, dL_dh * (1 - states[t + 1] ** 2))\n",
    "            \n",
    "        # Applying momentum and normalizing\n",
    "        # Calculating exponential averages\n",
    "        normalization_factor = 1 - (self.beta ** min(self.minibatches, 100))\n",
    "        self.dWhh = (self.beta * self.dWhh + (1 - self.beta) * (dL_dWhh)) / normalization_factor\n",
    "        self.dWhx = (self.beta * self.dWhx + (1 - self.beta) * (dL_dWhx)) / normalization_factor\n",
    "        self.dWyh = (self.beta * self.dWyh + (1 - self.beta) * (dL_dWyh)) / normalization_factor\n",
    "        self.dbh  = (self.beta * self.dbh + (1 - self.beta) * (dL_dbh)) / normalization_factor\n",
    "        self.dby  = (self.beta * self.dby + (1 - self.beta) * (dL_dby)) / normalization_factor\n",
    "\n",
    "        # Clipping the gradients for exploding gradients\n",
    "        for updates in [self.dWhh, self.dWhx, self.dWyh, self.dbh, self.dby]:\n",
    "            np.clip(updates, -1, 1, out=updates)\n",
    "\n",
    "        # Updating the weights and biases\n",
    "        self.Whh -= self.learning_rate * self.dWhh\n",
    "        self.Whx -= self.learning_rate * self.dWhx\n",
    "        self.Wyh -= self.learning_rate * self.dWyh\n",
    "        self.bh  -= self.learning_rate * self.dbh\n",
    "        self.by  -= self.learning_rate * self.dby\n",
    "\n",
    "    def train(self, train_X, train_Y, test_X, test_Y, epochs, batch_size=32):\n",
    "        # Checking train\n",
    "        train_X = np.array(train_X)\n",
    "        train_Y = [int(target) for target in train_Y]\n",
    "        training_size = train_X.shape[0]\n",
    "        assert(training_size == len(train_Y))\n",
    "        assert(train_X.shape[1] == self.sentence_length)\n",
    "        assert(train_X.shape[2] == self.input_dim)\n",
    "        \n",
    "        # Checking test\n",
    "        test_X = np.array(test_X)\n",
    "        test_Y = [int(target) for target in test_Y]\n",
    "        testing_size = test_X.shape[0]\n",
    "        assert(testing_size == len(test_Y))\n",
    "        assert(test_X.shape[1] == self.sentence_length)\n",
    "        assert(test_X.shape[2] == self.input_dim)\n",
    "\n",
    "        # Conforming batch size to a maximum of training_size / 2\n",
    "        batch_size = min(batch_size, training_size / 2)\n",
    "\n",
    "        # Transposing the testing data\n",
    "        test_X = np.transpose(test_X, (1, 2, 0))\n",
    "\n",
    "        # Array to store metrics\n",
    "        losses = []\n",
    "        correct_ans = []\n",
    "        log_frequency = max(int(float(epochs) / 100), 1)\n",
    "\n",
    "        # Splitting the training data into batches of size batchsize\n",
    "        batches = int(math.ceil(float(training_size) / batch_size))\n",
    "        batch_training_X = []\n",
    "        batch_training_Y = []\n",
    "\n",
    "        # Splitting training data into batches\n",
    "        for batch in range(batches):\n",
    "            start_index = batch_size * batch\n",
    "            end_index = min(batch_size * (batch + 1), training_size)\n",
    "            temp_batch_size = end_index - start_index\n",
    "            batch_train_X = train_X[start_index: end_index]\n",
    "\n",
    "            # Creating an np array of size (batch_size, max_batch_length, self.hidden_dim)\n",
    "            batch_train_X = np.array(batch_train_X).reshape(\n",
    "                (temp_batch_size, self.sentence_length, self.input_dim))\n",
    "            batch_train_Y = np.array(train_Y[start_index: end_index])\n",
    "            batch_training_X.append(batch_train_X)\n",
    "            batch_training_Y.append(batch_train_Y)\n",
    "\n",
    "        # Checking if the correct number of batches were inserted\n",
    "        assert(len(batch_training_X) == batches)\n",
    "\n",
    "        # Deleting variables to clear RAM\n",
    "        del train_X, train_Y\n",
    "\n",
    "        # Training the net\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            num_correct = 0\n",
    "\n",
    "            # Iterating through each training batch\n",
    "            for batch in range(batches):\n",
    "                # Picking out one batch of training samples\n",
    "                train_X = batch_training_X[batch]\n",
    "                train_Y = batch_training_Y[batch]\n",
    "\n",
    "                # Train_X from (batch_size, sentence_length, self.input_dim) \n",
    "                # to (sentence_length, self.input_dim, batch_size)\n",
    "                train_X = np.transpose(train_X, (1, 2, 0))\n",
    "\n",
    "                # Feed Forwarding\n",
    "                preds, states = self.forward(train_X)\n",
    "\n",
    "                loss -= np.sum(np.log([pred[train_Y[index]] for index, pred in enumerate(preds.T)]))\n",
    "                num_correct += np.sum(np.argmax(preds, axis=0) == train_Y)\n",
    "\n",
    "                # Back propagating the error\n",
    "                self.backward(train_X, train_Y, preds, states, batch_size)\n",
    "\n",
    "                # Updating the mini batch number\n",
    "                self.minibatches += 1\n",
    "\n",
    "            # Appending loss to training data\n",
    "            losses.append(loss)\n",
    "            correct_ans.append((float(num_correct) / training_size) * 100)\n",
    "\n",
    "            # Printing loss and number of correctly classified values\n",
    "            if(epoch % log_frequency == 0):\n",
    "                train_loss = round(loss / training_size, 3)\n",
    "                train_accuracy = round(float(num_correct) / training_size, 3)\n",
    "\n",
    "                # Resetting loss and correct answers\n",
    "                loss = 0\n",
    "                num_correct = 0\n",
    "\n",
    "                # Feed Forwarding\n",
    "                preds, states = self.forward(test_X)\n",
    "\n",
    "                # Calculating the loss and correct classifications\n",
    "                loss -= np.sum(np.log([pred[test_Y[index]] for index, pred in enumerate(preds.T)]))\n",
    "                num_correct += np.sum(np.argmax(preds, axis=0) == test_Y)\n",
    "                \n",
    "                test_loss = round(loss / testing_size, 3)\n",
    "                test_accuracy = round(float(num_correct) / testing_size, 3)\n",
    "                \n",
    "                print(\n",
    "                    f\"Epoch: {epoch+1:03d}/{epochs:03d} | \"\n",
    "                    f\"train loss: {train_loss:.3f} | test loss: {test_loss:.3f} | \"\n",
    "                    f\"train acc: {train_accuracy:.3f} | test acc: {test_accuracy:.3f}\"\n",
    "                )\n",
    "\n",
    "        return losses, correct_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 23170\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 2\n",
    "\n",
    "rnn = VanillaRNN(EMBEDDING_DIM,\n",
    "                 OUTPUT_DIM,\n",
    "                 SENTENCE_LENGTH,\n",
    "                 hidden_dim=HIDDEN_DIM,\n",
    "                 learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import io\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "class GloveEmbeddings:\n",
    "    def __init__(self, sentence_length, embedding_dim=50, remove_stop_words=False):\n",
    "        self.sentence_length = sentence_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.remove_stop_words = remove_stop_words\n",
    "\n",
    "        glove_path = \"/home/kate/Desktop/projects/ML_DL_experiments/nn/glove.6B.50d.txt\"\n",
    "        self.word2vec = self.load_glove_model(glove_path)\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.outlier_words = []\n",
    "\n",
    "    def load_glove_model(self, gloveFile):\n",
    "        f = io.open(gloveFile, 'r', encoding=\"utf-8\")\n",
    "        model = {}\n",
    "        for line in f:\n",
    "            splitLine = line.split()\n",
    "            word = splitLine[0]\n",
    "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "            model[word] = embedding\n",
    "        print(f\"{len(model)} words loaded!\")\n",
    "        return model\n",
    "\n",
    "    def remove_punctuation(self, sentence):\n",
    "        output = \"\"\n",
    "        for symbol in sentence:\n",
    "            # Replacing all special symbols with a space \n",
    "            if (symbol in string.punctuation):\n",
    "                output += \" \"\n",
    "            else:\n",
    "                output +=  symbol\n",
    "        return output\n",
    "\n",
    "    def tokenize(self, input_sentence):\n",
    "        sentence = self.remove_punctuation(input_sentence)\n",
    "        words = word_tokenize(sentence)\n",
    "        words = [word.lower() for word in words]\n",
    "\n",
    "        if self.remove_stop_words:\n",
    "            new_words = []\n",
    "            for word in words:\n",
    "                if(word not in self.stopwords):\n",
    "                    new_words.append(word)\n",
    "            return new_words\n",
    "        else:\n",
    "            return words\n",
    "        \n",
    "    def pad_sentence(self, words):\n",
    "        padding = [0 for _ in range(self.embedding_dim)]\n",
    "        sentence_length = len(words)\n",
    "        if(sentence_length < self.sentence_length):\n",
    "            for _ in range(self.sentence_length - sentence_length):\n",
    "                words.append(padding)\n",
    "        return words\n",
    "\n",
    "    def trim_sentence(self, words):\n",
    "        if(len(words) > self.sentence_length):\n",
    "            return words[:self.sentence_length]\n",
    "        return words\n",
    "\n",
    "    def create_input(self, sentence):\n",
    "        words = self.tokenize(sentence)\n",
    "        inputs = []\n",
    "\n",
    "        for index, word in enumerate(words):\n",
    "            try:\n",
    "                inputs.append(self.word2vec[word])\n",
    "            except:\n",
    "                self.outlier_words.append(word)\n",
    "                inputs.append(np.zeros(self.embedding_dim))\n",
    "\n",
    "        padded_inputs = self.pad_sentence(inputs)\n",
    "        trimmed_inputs = self.trim_sentence(padded_inputs)\n",
    "        inputs = np.array(trimmed_inputs).reshape(\n",
    "            self.sentence_length, self.embedding_dim)\n",
    "        return inputs\n",
    "\n",
    "    def get_data_from_list(self, sentences):\n",
    "        data_size = len(sentences)\n",
    "        output_data = np.array([self.create_input(sentence) for sentence in sentences])\n",
    "        assert(output_data.shape == (data_size, self.sentence_length, self.embedding_dim))\n",
    "        return np.array(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 words loaded!\n"
     ]
    }
   ],
   "source": [
    "# Creating an embeddings class object\n",
    "embedding = GloveEmbeddings(SENTENCE_LENGTH,\n",
    "                            embedding_dim=EMBEDDING_DIM,\n",
    "                            remove_stop_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df['TEXT_COLUMN_NAME'], df['LABEL_COLUMN_NAME']\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perparing the input data\n",
    "train_X = embedding.get_data_from_list(train_X)\n",
    "test_X = embedding.get_data_from_list(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/015 | train loss: 0.725 | test loss: 0.702 | train acc: 0.505 | test acc: 0.530\n",
      "Epoch: 002/015 | train loss: 0.696 | test loss: 0.690 | train acc: 0.533 | test acc: 0.550\n",
      "Epoch: 003/015 | train loss: 0.688 | test loss: 0.684 | train acc: 0.551 | test acc: 0.566\n",
      "Epoch: 004/015 | train loss: 0.682 | test loss: 0.679 | train acc: 0.564 | test acc: 0.574\n",
      "Epoch: 005/015 | train loss: 0.676 | test loss: 0.672 | train acc: 0.576 | test acc: 0.588\n",
      "Epoch: 006/015 | train loss: 0.662 | test loss: 0.631 | train acc: 0.607 | test acc: 0.657\n",
      "Epoch: 007/015 | train loss: 0.620 | test loss: 0.615 | train acc: 0.668 | test acc: 0.680\n",
      "Epoch: 008/015 | train loss: 0.609 | test loss: 0.606 | train acc: 0.682 | test acc: 0.689\n",
      "Epoch: 009/015 | train loss: 0.598 | test loss: 0.605 | train acc: 0.694 | test acc: 0.688\n",
      "Epoch: 010/015 | train loss: 0.596 | test loss: 0.600 | train acc: 0.695 | test acc: 0.691\n",
      "Epoch: 011/015 | train loss: 0.594 | test loss: 0.602 | train acc: 0.696 | test acc: 0.687\n",
      "Epoch: 012/015 | train loss: 0.595 | test loss: 0.601 | train acc: 0.697 | test acc: 0.690\n",
      "Epoch: 013/015 | train loss: 0.609 | test loss: 0.618 | train acc: 0.684 | test acc: 0.661\n",
      "Epoch: 014/015 | train loss: 0.605 | test loss: 0.585 | train acc: 0.680 | test acc: 0.700\n",
      "Epoch: 015/015 | train loss: 0.583 | test loss: 0.587 | train acc: 0.703 | test acc: 0.697\n"
     ]
    }
   ],
   "source": [
    "# Training RNN\n",
    "losses, correct_values = rnn.train(train_X, train_Y,\n",
    "                                   test_X, test_Y,\n",
    "                                   NUM_EPOCHS,\n",
    "                                   batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация Pytorch RNN\n",
    "\n",
    "[Inspiration code source](https://github.com/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_imdb-glove.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        # self.rnn = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_length):\n",
    "        embedded = self.embedding(text)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, text_length)\n",
    "        \n",
    "        packed_output, hidden = self.rnn(packed)\n",
    "        # packed_output, (hidden, cell) = self.rnn(packed)\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0)).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_binary_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(data_loader):\n",
    "            text, text_lengths = batch_data.TEXT_COLUMN_NAME\n",
    "            logits = model(text, text_lengths)\n",
    "            predicted_labels = (torch.sigmoid(logits) > 0.5).long()\n",
    "            num_examples += batch_data.LABEL_COLUMN_NAME.size(0)\n",
    "            correct_pred += (predicted_labels == batch_data.LABEL_COLUMN_NAME.long()).sum()\n",
    "        return correct_pred.float()/num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [39:04<00:00,  8.81s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001/015 | Train loss: 0.620 | Valid loss: 0.655 | Train acc: 0.659 | Valid acc: 0.648 | 40.04 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [33:40<00:00,  7.60s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "002/015 | Train loss: 0.662 | Valid loss: 0.672 | Train acc: 0.651 | Valid acc: 0.637 | 74.68 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [34:29<00:00,  7.78s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "003/015 | Train loss: 0.621 | Valid loss: 0.664 | Train acc: 0.646 | Valid acc: 0.633 | 110.14 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [34:42<00:00,  7.83s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "004/015 | Train loss: 0.569 | Valid loss: 0.650 | Train acc: 0.720 | Valid acc: 0.702 | 145.79 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [35:53<00:00,  8.10s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "005/015 | Train loss: 0.573 | Valid loss: 0.658 | Train acc: 0.716 | Valid acc: 0.706 | 182.64 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [33:50<00:00,  7.63s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "006/015 | Train loss: 0.622 | Valid loss: 0.633 | Train acc: 0.731 | Valid acc: 0.710 | 217.44 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [34:07<00:00,  7.70s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "007/015 | Train loss: 0.549 | Valid loss: 0.625 | Train acc: 0.720 | Valid acc: 0.699 | 252.51 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [36:04<00:00,  8.14s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "008/015 | Train loss: 0.453 | Valid loss: 0.640 | Train acc: 0.760 | Valid acc: 0.736 | 289.51 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [33:42<00:00,  7.60s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "009/015 | Train loss: 0.505 | Valid loss: 0.602 | Train acc: 0.773 | Valid acc: 0.748 | 324.19 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [33:42<00:00,  7.60s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "010/015 | Train loss: 0.478 | Valid loss: 0.609 | Train acc: 0.778 | Valid acc: 0.750 | 358.87 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [33:55<00:00,  7.65s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "011/015 | Train loss: 0.489 | Valid loss: 0.617 | Train acc: 0.788 | Valid acc: 0.762 | 393.77 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [33:47<00:00,  7.62s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "012/015 | Train loss: 0.483 | Valid loss: 0.605 | Train acc: 0.790 | Valid acc: 0.756 | 428.50 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [34:38<00:00,  7.81s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "013/015 | Train loss: 0.660 | Valid loss: 0.667 | Train acc: 0.546 | Valid acc: 0.537 | 464.10 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [38:26<00:00,  8.67s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "014/015 | Train loss: 0.470 | Valid loss: 0.616 | Train acc: 0.775 | Valid acc: 0.750 | 503.49 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [38:04<00:00,  8.59s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "015/015 | Train loss: 0.425 | Valid loss: 0.565 | Train acc: 0.785 | Valid acc: 0.752 | 542.96 min\n",
      "Total Training Time: 542.96 min\n",
      "Test accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, batch_data in enumerate(tqdm(train_loader)):\n",
    "        text, text_lengths = batch_data.TEXT_COLUMN_NAME\n",
    "        label = batch_data.LABEL_COLUMN_NAME\n",
    "        \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(text, text_lengths)\n",
    "        train_loss = F.binary_cross_entropy_with_logits(logits, label)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loss.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "\n",
    "    ### VALIDATION\n",
    "    for batch_idx, batch_data in enumerate(valid_loader):\n",
    "        text, text_lengths = batch_data.TEXT_COLUMN_NAME\n",
    "        label = batch_data.LABEL_COLUMN_NAME\n",
    "        logits = model(text, text_lengths)\n",
    "        valid_loss = F.binary_cross_entropy_with_logits(logits, label)\n",
    "\n",
    "    ### LOGGING\n",
    "    with torch.set_grad_enabled(False):\n",
    "        train_acc = compute_binary_accuracy(model, train_loader, DEVICE)\n",
    "        valid_acc = compute_binary_accuracy(model, valid_loader, DEVICE)\n",
    "    \n",
    "    print(\n",
    "        f'{epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "        f'Train loss: {train_loss:.3f} | Valid loss: {valid_loss:.3f} | '\n",
    "        f'Train acc: {train_acc:.3f} | Valid acc: {valid_acc:.3f} | '\n",
    "        f'{(time.time() - start_time)/60:.2f} min'\n",
    "    )\n",
    "\n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_binary_accuracy(model, test_loader, DEVICE):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура LSTM (Long Short Term Memory)\n",
    "\n",
    "* [Understanding LSTM Networks - colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "Long Short Term Memory (LSTM) – подкласс рекуррентных нейронных сетей, которые были предложены в 1997 Хохрайтером и Шмидхубером. Они гораздо лучше классических RNN справляются с \"запоминанием\" долгосрочных зависимостей.\n",
    "\n",
    "Архитектура LSTM представляет из себя усложнение конструкции внутри ячейки RNN: вместо одного слоя в элементе содержится 4 слоя с разными функциями (желтые квадраты). Также каждая LSTM-ячейка имеет **состояние**, на которые влияют три вида узлов, называемых **гейтами**: входной (input gate), забывающий (forget gate) и выходной (output gate). И еще каждая ячейка имеет 3 входа: входной вектор $x_t$, состояние ячейки $c_t$ и скрытое состояние $h_t$ (во время $t$).\n",
    "\n",
    "<img src=\"pictures/lstm.png\" width=600 height=600 />\n",
    "\n",
    "Вычисления внутри LSTM-ячейки: для входа $x_t$ и значения вектора скрытого состояния на предыдущем шаге $h_{t-1}$ и вектора собственного состояния яейки на предыдущем шаге $c_{t-1}$:\n",
    "* $c_t' = \\text{tanh}(W_{xc} x_t + W_{hc} h_{t-1} + b_{c'})$ - candidate cell state\n",
    "* $i_t = \\sigma (W_{xi} x_t + W_{hi} h_{t-1} + b_{i})$ - input gate\n",
    "* $f_t = \\sigma (W_{xf} x_t + W_{hf} h_{t-1} + b_{f})$ - forgate gate\n",
    "* $o_t = \\sigma (W_{xo} x_t + W_{ho} h_{t-1} + b_{o})$ - output gate\n",
    "* $c_t = f_t \\cdot c_{t-1} + i_t \\cdot c_t'$ - cell state\n",
    "* $h_t = o_t \\cdot \\text{tanh}(c_t)$ - block output\n",
    "\n",
    "Разные слои LSTM отвечают за разные задачи:\n",
    "* Один слой вычисляет, насколько на данном шаге ему нужно забыть предыдущую информацию \n",
    "* Другой слой вычисляет, насколько ему интересна новая информация, пришедшая с сигналом\n",
    "* На третьем слое вычисляется линейная комбинация памяти и наблюдения с только вычисленными весами для каждой из компонент. Так получается новое состояние памяти, которое в таком же виде передаётся далее.\n",
    "\n",
    "**Основная идея LSTM**\n",
    "* Состояние ячейки — горизонтальная линия, проходящая через верхнюю часть диаграммы\n",
    "* При обучении нейронной сети параметром является размер ячейки\n",
    "* 3 гейта регулируют то, какую часть информации в состоянии ячейки нужно забыть, а какую запомнить\n",
    "* Сигмоида принимает значения от 0 до 1, описывающие, сколько каждого компонента должно быть пропущено: 0 - это ничего не пропускать, 1 - забыть все\n",
    "\n",
    "<img src=\"pictures/lstm1.png\" width=600 height=600 />\n",
    "\n",
    "**Первый шаг LSTM**\n",
    "* Первый слой вычисляет, насколько на данном шаге ему нужно забыть предыдущую информацию, какую информацию можно выбросить из ячейки (forget gate layer):\n",
    "* Результат будет перемножен со значениями ячейки. Некоторые значения могут обнулиться  или снизить свой вклад\n",
    "\n",
    "<img src=\"pictures/lstm2.png\" width=600 height=600 />\n",
    "\n",
    "**Внесение новой информации в ячейку**\n",
    "* Второй слой вычисляет, насколько ему интересна новая информация, пришедшая с сигналом.\n",
    "* Этап состоит из двух частей:\n",
    "    1. Сначала сигмоидальный слой (input layer gate) определяет, какие значения следует обновить\n",
    "    2. Затем tanh-слой строит вектор новых значений-кандидатов, которые можно добавить в состояние ячейки\n",
    "\n",
    "<img src=\"pictures/lstm3.png\" width=600 height=600 />\n",
    "\n",
    "**Обновление ячейки памяти**\n",
    "* На третьем слое вычисляется линейная комбинация памяти и наблюдения с только вычисленными весами для каждой из компонент:\n",
    "    * Старое нужно забыть в соответствии с $f_t$ \n",
    "    * Прибавляем новые значения, умноженные на коэффициент обновления $i_t$\n",
    "\n",
    "<img src=\"pictures/lstm4.png\" width=600 height=600 />\n",
    "\n",
    "**Выработка выходного значения (выходной слой:)**\n",
    "* Выходные данные основываются на состоянии ячейки\n",
    "* Применяется еще один фильтр (сигмоида), чтобы решить какую информацию нужно использовать\n",
    "\n",
    "<img src=\"pictures/lstm5.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модификации LSTM\n",
    "\n",
    "**Peephole connections**\n",
    "\n",
    "<img src=\"pictures/peephole_lstm.png\" width=600 height=600 />\n",
    "\n",
    "**Coupled forget and input gates**\n",
    "\n",
    "<img src=\"pictures/lstm_mod2.png\" width=600 height=600 />\n",
    "\n",
    "### Достоинства LSTM\n",
    "\n",
    "Все типы рекуррентных нейронных сетей (RNN, LSTM и GRU) страдают от того, что последние элементы последовательности влияют на результат больше, чем первые. Однако, LSTM гораздо лучше справляются с проблемой исчезающих градиентов, чем обычные RNN.\n",
    "\n",
    "> Constatnt error carousel (рекурсивное вычисление без нелинейности)\n",
    "\n",
    "### Рекомендации по настройке параметров LSTM\n",
    "* Использовать gradient clipping (clipnorm или clipvalue) для предотвращения проблемы взрывающихся градиентов\n",
    "* Использовать нормализацию по батчам и инициализацию весов Xavier\n",
    "* Инициализировать свободный член $b_f$ большими случайными числами (от 1 до 2), чтобы constatnt error carousel \"работала\"\n",
    "* Использовать разные типы регуляризации: L1, L2, dropout \n",
    "* Ограничивать количество параметров для небольших выборок\n",
    "* Использовать функцию активации softsign (не softmax) через tanh (она быстрее и менее подвержена насыщению\n",
    "* Использовать RMSProp, Adam, AdaGrad или Nesterov momentum, Adagrad иногда тоже неплохой выбор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура GRU\n",
    "\n",
    "GRU - вариант LSTM со значительно упрощенной архитектурой: в классической архитектуре LSTM целых 8 матриц весов + еще 3 матрицы, в случае использования peephole. Если же мы будем использовать модификацию LSTM, в которой \"свяжем\" входной и забывающий гейты, то получим архитектуру GRU: gated recurrent union.\n",
    "\n",
    "<img src=\"pictures/gru.png\" width=600 height=600 />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
