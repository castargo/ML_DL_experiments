{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сверточные нейронные сети\n",
    "\n",
    "* [Wiki - Свёрточная нейронная сеть](https://ru.wikipedia.org/wiki/%D0%A1%D0%B2%D1%91%D1%80%D1%82%D0%BE%D1%87%D0%BD%D0%B0%D1%8F_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C)\n",
    "* [Лекция. Сверточные нейронные сети - Deep Learning School](https://www.youtube.com/watch?v=HpKGv-kYurk)\n",
    "* [Как работает сверточная нейронная сеть (CNN)](https://neurohive.io/ru/osnovy-data-science/glubokaya-svertochnaja-nejronnaja-set/)\n",
    "* [Визуализация CNN](https://animatedai.github.io/)\n",
    "* Компьютерное зрение. Теория и алгоритмы - Рейнхард Клетте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мотивация к применению\n",
    "\n",
    "В [серии конспектов](https://github.com/castargo/ML_DL_experiments/tree/main/nn/basics) про основы полносвязных нейронных сетей упомяналось, что по [теореме Хорника](https://en.wikipedia.org/wiki/Universal_approximation_theorem) известно, что нейронная сеть с одним скрытым слоем является универсальным аппроксиматором, то есть может аппроксимировать непрерывную функцию любого типа. Тогда в чем заключается мотивация использования, например, сверточных нейронных сетей?\n",
    "\n",
    "У полносвязных нейронных сетей есть особенности, которые затрудняют обучении при работе с данными, имеющими структуру: картинками, аудио- и видеозаписями, последовательностями и т.п. Вспомним [пример](https://github.com/castargo/ML_DL_experiments/blob/main/nn/basics/04_gradient_descent.ipynb) с обучением полносвязной нейронной сети на датасете MNIST. Там мы \"разворачивали\" изображение в массив следующим образом:\n",
    "\n",
    "```python\n",
    "image_tensor.flatten().numpy()\n",
    "```\n",
    "\n",
    "Такой способ представления не учитывает особенности структуры (пиксельной сетки) изображения. Полносвязная нейронная сеть выучивает зависимость \"развернутого\" изображения, создавая некоторую маску, которую затем пытается предсказать на других изображениях-векторах. Тогда если изображения в тестовой выборке будут отличаться от тренировочной (например, объект будет находиться в другой части изображения, будет меньше или больше и т.п.), полносвязная нейронная сеть не сможет сделать корректный прогноз. Мы могли бы добавить в обучающую выборку больше примеров, чтобы у каждого класса было больше специфичных масок, но тогда и нейронная сеть должна иметь больше обучаемых параметров. \n",
    "\n",
    "В основе сверточных нейронных сетей лежит идея, которая помогает эффективно решить обе эти проблемы: будем сохранять сеточную структуру изображения, но использовать некоторое \"скользящее окно\", называемое **сверткой** для локализации зависимостей на изображении. Так мы преодолеем проблему перемещающегося объекта на изображении, ведь **операция свертки инвариантна** относительно ее местоположения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предтечи сверточных нейронных сетей\n",
    "\n",
    "Идея операции свертки во многом была сформирована при изучении зрительного аппарата человека. Первые работы по изучению зрительной коры головного мозга были опубликованы Хьюбелом и Визелем в 1950-х. В 1959 году исследователи предложили модель, согласно которой, существует два вида клеток в первичной зрительной коре: простая и сложная клетка, расположенные каскадно. Опираясь на эту идею, в 1980 году Кунихико Фукусима предложил модель обучения без учителя для распознавания образов, которая называлась [неокогнитрон](https://ru.wikipedia.org/wiki/%D0%9D%D0%B5%D0%BE%D0%BA%D0%BE%D0%B3%D0%BD%D0%B8%D1%82%D1%80%D0%BE%D0%BD).\n",
    "\n",
    "Со временем сформировалась область computer science, которорая стала называться **компьютерным зрением**. Существует множество \"традиционных\" подходов, связанных с областью цифровой обработки сигналов, в которых изображения рассматривались как двумерные сигналы, на которые, например, применяли различные [фильтры](https://ai.stanford.edu/~syyeung/cvweb/tutorial1.html): линейный фильтр (кросс-корреляция или свертка), фильтр Гаусса для размытия и снижения уровня шума, фильтры Габора для выделения границ объектов на изображениях, фильтры Хаара для распознавания объектов на изображении (для алгоритма Виолы-Джонса) и т.п. [Смотри отличную лекция Андрея Савченко.](https://www.youtube.com/watch?v=L3EBxvSC20M&t=1390s&ab_channel=QAHub)\n",
    "\n",
    "[Сверточные нейронные сети](https://ru.wikipedia.org/wiki/%D0%A1%D0%B2%D1%91%D1%80%D1%82%D0%BE%D1%87%D0%BD%D0%B0%D1%8F_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C) были предложены в 1989 году группой исследователей, которую возглавлял Ян Лекун, в статье [Backpropagation Applied to Handwritten Zip Code Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf). Там исследователи впервые применили подход на предке датасета MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## В общих чертах: сверточные нейронные сети в задачах, связанных с обработкой изображений\n",
    "\n",
    "В датасете MNIST каждое изображение представлено одномерным тензором размерности ```torch.Size([1, 28, 28])```, в каждой позиции стоит число от 0 до 1 в зависимости от интенсивности цвета в данном пикселе. Изображение является черно-белым.\n",
    "\n",
    "Чтобы представить цветное изображение, необходим многомерный тензор (например, ```torch.Size([3, 28, 28])```): каждая матрица задает интенсивность цвета в данном пикселе изображения в RGB-представлении. То есть каждому пикселю изображения сопоставляется одномерный тензор, описывающий цветовые свойства этого пикселя, и чьи компоненты называются **каналами**.\n",
    "\n",
    "> Чтобы закодировать 8-битное изображение, можно использовать числа от 0 до 255: $2^8 = 256$\n",
    "\n",
    "Идея свёрточных нейронных сетей заключается в чередовании свёрточных слоёв (**convolution layers**) и субдискретизирующих слоёв (**pooling layers, subsampling**). Структура сверточной нейронной сети является однонаправленной (без обратных связей) и многослойной. На вход подается изображение (многомерный тензор), которое проходит через серию сверточных слоев, в которых чередуются операции свертки и субдискретизации (пулинг). Такой подход с чередованием слоев позволяет выделять из изображений сложные зависимости и низкоуровневые признаки (feature map). Как правило, после нескольких слоев свертки и пулинга многмерный тензор приводится к вектору или скаляру. Также на выходе сверточной нейронной сети может быть несколько полносвязных слоев.\n",
    "\n",
    "<img src=\"pictures/CNN.jpg\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сверточный слой (convolutional layer)\n",
    "\n",
    "Принцип работы сверточного слоя заключается в применении операции свертки с некоторым **ядром свертки** к входному изображению. Если изображение является цветным, то есть оно представлено несколькими каналами (является многомерным тензором), то и ядро свертки тоже будет многомерным тензором, где у каждого канала будет своя маска (фильтр). К слову, количество каналов после сверточного слоя может увеличиться:\n",
    "\n",
    "$$N \\text{ каналов на входе: свертка размера } N \\times M \\rightarrow M \\text{ каналов на выходе}$$\n",
    "\n",
    "**Ядро свертки** - это, как правило, небольшая матрица, чьи коэффициенты не задаются заранее, а являются обучаемыми параметрами (весами), оптимизируемыми в процессе схождения градиентного спуска (не забываем про bias). На соответствующем слое инициализируется некоторое ядро свертки, которое затем двигают по всему обрабатываемому слою. При этом разные ядра могут **кодировать разные элементы изображения**: участки, линии, дуги и т.п. Проход каждым набором весов формирует свой собственный экземпляр карты признаков (**feature map**) - выходной тензор, делая нейронную сеть многоканальной (много независимых карт признаков на одном слое). Следущий слой, получающий на вход результаты свертки текущего слоя, будет \"кодировать\" более высокоуровневую информацию, таким образом используя пространственные корреляции на изображениях.\n",
    "\n",
    "> Local respective fields (пятно восприятия)\n",
    "\n",
    "Свертка - некоторое линейное преобразование входных данных (скалярное умножение элементов матрицы окна на матрицы свертки, а не произведение матриц). Легче всего механизм ее работы рассмотреть на одноканальном изображении (матрице). Тогда алгоритм применения свертки к изображению будет следующим:\n",
    "1. Изображение разбивается на фрагменты-матрицы\n",
    "2. Каждый фрагмент \"сворачивается\" при помощи ядра свертки (в этом случае тоже матрицы)\n",
    "3. Значения свертки последовательно переносятся в выходную матрицу меньшего размера\n",
    "\n",
    "На изображении ниже:\n",
    "* input - вход сверточного слоя\n",
    "* output - выход сверточного слоя (feature map)\n",
    "* небольшая матрица - ядро свертки\n",
    "\n",
    "<img src=\"pictures/convolution_inp_out.png\" width=600 height=600 />\n",
    "\n",
    "А вообще, к значению ячейки при обучении нейронной сети добавляется еще и bias.\n",
    "\n",
    "#### Padding (отступ)\n",
    "\n",
    "На картинке выше после свертки мы получили выходной тензор того же размера, что и входной, при помощи трюка с добавлением фиктивной \"рамки\" толщины 1, дублирующей крайние элементы тензора. Вообще, такая \"рамка\" называется **padding** и может заполняться и другими значениями, например, нулями.\n",
    "\n",
    "#### Stride (шаг)\n",
    "\n",
    "Обязательно ли нам всегда двигать свертку только на один пиксель вправо-влево и вверх-вниз? Нет, шаг (**stride**) может быть любого размера, и тогда мы будем \"перескакивать\" колонки и строчки нашего изображения. Стоит ли двигать свертку на размер матрицы весов? Вообще, нет, ведь так можно пропустить полезные признаки на изображении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Слой пулинга (субдискретизирующий слой, subsampling, pooling layers)\n",
    "\n",
    "Для того, чтобы добиться хорошего перформанса от сверточных нейронных сетей, нам нужно научиться **эфективно \"снижать размерность\"** входного изображения, то есть \"просматривать\" полученные после свертки карты признаков и отбирать оттуда наиболее полезные. В этом помогает **операция пулинга (pooling, subsampling)**: \n",
    "* Задается параметр, который разбивает изображение на непересекающиеся группы признаков (например, квадраты размера 2×2 пикселя)\n",
    "* При помощи какой-то нелинейной (чаще всего) или линейной операции из каждой группы отбирается один признак (пиксель)\n",
    "* Из отобранных пикселей формируется выход слоя пулинга\n",
    "\n",
    "В результате операции пулинга мы \"сжимаем\" входное изображение, уплотняя непересекающиеся группы признаков. Можно выделить следующие достоинства данной операции:\n",
    "* Ускорение вычислений (изображение на выходе меньше, чем на входе)\n",
    "* Устойчивость сети к размеру входного изображения\n",
    "* Фильтрация малозначимых признаков помогает в борьбе с переобучением\n",
    "* Акцент на наличии признака на изображении, а не на его расположении\n",
    "\n",
    "Виды пулинга:\n",
    "* MaxPooling\n",
    "* MinPooling\n",
    "* AvgPooling\n",
    "* L2-нормирование\n",
    "\n",
    "Пример MaxPooling:\n",
    "\n",
    "<img src=\"pictures/pooling.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Объединяем слои в CNN\n",
    "\n",
    "Итак, мы определили слой свертки и слой пулинга, будем собирать из них (и не только) сверточную нейронную сеть. Есть несколько основных правил:\n",
    "* Последовательно применяем на входном изображении одну или более операций свертки, при этом, как правило, на каждом следующем слое увеличивается число каналов и уменьшается размерность изображения в каждом канале. Новый набор каналов представляет изображение с точки зрения абстрактных признаков, которые оно содержит.\n",
    "* После свертки почти всегда следует нелинейная функция активации, например ReLU. Часто даже по умолчанию считается, что функция активации встроена в слой свертки.\n",
    "* Слой пулинга, как правило, вставляется после слоя свёртки перед слоем следующей свёртки.\n",
    "* После нескольких итераций данные объединяются и передаются на многослойную полносвязную нейронную сеть (здесь нам уже не важна пространственная структура в данных). \n",
    "* На выходе из нейронной сети используется функция активации Softmax.\n",
    "\n",
    "Наиболее популярный способ обучения сверточных нейронных сетей - метод обратного распространения ошибки и его модификации: [Backpropagation In Convolutional Neural Networks](https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/)\n",
    "\n",
    "Что можно предпринять в случае переобучения?\n",
    "* Уменьшить число слоев\n",
    "* Уменьшить кол-во фильтров в сверточных слоях\n",
    "* Увеличить коэффициент дропаута"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution arithmetic\n",
    "\n",
    "[A guide to convolution arithmetic for deep learning](https://arxiv.org/pdf/1603.07285.pdf)\n",
    "\n",
    "#### Пример из [wiki](https://ru.wikipedia.org/wiki/%D0%A1%D0%B2%D1%91%D1%80%D1%82%D0%BE%D1%87%D0%BD%D0%B0%D1%8F_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C)\n",
    "\n",
    "Особенностью свёрточного слоя является сравнительно небольшое количество параметров, устанавливаемое при обучении. Так например, если исходное изображение имеет размерность 100×100 пикселей по 3-м каналам (это значит 30 000 входных нейронов), а свёрточный слой использует фильтры c ядром 3×3 пикселя с выходом на 6 каналов, тогда в процессе обучения определяется только 9 весов ядра, однако по всем сочетаниям каналов, то есть 9×3×6=162 (9 весов, 3 канала входных, 6 каналов выходных), в таком случае данный слой требует нахождения только **162 параметров**, что существенно меньше количества искомых параметров полносвязной нейронной сети.\n",
    "\n",
    "Замечание: видимо, в примере нет bias.\n",
    "\n",
    "#### Размер тензора после свертки\n",
    "* Входное изображение 36x36x3 пикселей \n",
    "* Свертка 8 фильтров размера 3x3x3, padding=1, stride=2\n",
    "\n",
    "Для изображения, у которого $W = H$ ([смотри документацию](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)):\n",
    "\n",
    "$$W_{\\text{out}} = [\\frac{(W_{\\text{in}} + 2 \\text{padding} - (\\text{kernel size} - 1) - 1)}{\\text{stride}} + 1]$$\n",
    "\n",
    "$$\\text{math.floor}( \\frac{(36 + 2 - (3 - 1) - 1)}{2} + 1)= 18$$\n",
    "\n",
    "Размер тензора после свертки: 18x18x8\n",
    "\n",
    "#### Параметры в слое\n",
    "* Входное изображение 128x128x3\n",
    "* Свертка 3x3, padding=1, stride=1\n",
    "* Кол-во выходных каналов=10\n",
    "\n",
    "$$\\text{in_channels * out_channels * kernel_size + bias}$$\n",
    "\n",
    "$$\\text{bias = out_channels}$$\n",
    "\n",
    "Тогда кол-во настраиваемых параметров: 3 * 10 * 9 + 10 = 280\n",
    "\n",
    "```\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet-5 PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выгрузим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST(root='data/',\n",
    "                    train=True,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.Resize((32,32)),\n",
    "                        transforms.ToTensor(), \n",
    "                        transforms.Normalize(mean=(0.1307,), std=(0.3081,))]),\n",
    "                    download=True)\n",
    "\n",
    "mnist_test = MNIST(root='data/',\n",
    "                   train=False,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.Resize((32,32)),\n",
    "                        transforms.ToTensor(), \n",
    "                        transforms.Normalize(mean=(0.1307,), std=(0.3081,))]),\n",
    "                    download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "train_loader = DataLoader(mnist_train, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(mnist_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 32, 32]), 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor, label = mnist_test[0]\n",
    "image_tensor.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image_tensor[0, 0:32, 0:32]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/lenet5.png\" width=700 height=700 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU())\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU())\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(16 * 5 * 5, 120),\n",
    "            nn.ReLU())\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU())\n",
    "        self.out = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], loss: 1.92073\n",
      "Epoch [2/10], loss: 0.7912\n",
      "Epoch [3/10], loss: 0.40254\n",
      "Epoch [4/10], loss: 0.30409\n",
      "Epoch [5/10], loss: 0.26429\n",
      "Epoch [6/10], loss: 0.24407\n",
      "Epoch [7/10], loss: 0.23159\n",
      "Epoch [8/10], loss: 0.22342\n",
      "Epoch [9/10], loss: 0.21756\n",
      "Epoch [10/10], loss: 0.21346\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCH):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{NUM_EPOCH}], loss: {round(loss.item(), 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метрики модели на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97.2 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test accuracy: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## История архитектур CNN на примере ILSVRC\n",
    "\n",
    "[CNN Architectures: LeNet, AlexNet, VGG, GoogLeNet, ResNet and more...](https://medium.com/analytics-vidhya/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5)\n",
    "\n",
    "Первые работы, связанные со сверточными нейронными сетями, были применены на наборе данных с рукописными цифрами MNIST, и уже скоро на нем было достигнуто достаточно высокое качество классификации. Новым челленджем для исследователей на десятилетия стал набор данных ImageNet. В рамках этого проекта проводился ежегодный конкурс программного обеспечения ImageNet Large Scale Visual Recognition Challenge (ILSVRC), в котором исследователи соревновались в максимально достигаемом качестве.\n",
    "\n",
    "| Год | Архитектура | Исследователи | Место ILSVRC | Top-5 error rate | Количество параметров |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| 1998 | LeNet | Yann LeCun |  |  | 60K (7 слоев) |\n",
    "| 2012 | AlexNet | Alex Krizhevsky, Geoffrey Hinton, Ilya Sutskever | 1 | 15.3% | 60M (8 слоев) |\n",
    "| 2013 | ZFNet | Matthew Zeiler, Rob Fergus | 1 | 14.8% |  |\n",
    "| 2014 | VGG | Simonyan, Zisserman | 2 | 7.3% | 138M (19 слоев) |\n",
    "| 2014 | GoogLeNet | Google | 1 | 6.67% | 4M (22 слоя) |\n",
    "| 2015 | ResNet | Kaiming He | 1 | 3.6% | (152 слоя) |\n",
    "\n",
    "### LeNet-5 (1998, Ян Лекун)\n",
    "\n",
    "[Gradient-based learning applied to document recognition (LeCun, Bottou, Bengio, Haffner)](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf).\n",
    "\n",
    "Это архитектура из семи слоев использовалась для классификации рукописных чисел на ч/б изображениях размером 32x32 пикселей.\n",
    "\n",
    "<img src=\"pictures/lenet5.png\" width=700 height=700 />\n",
    "\n",
    "### AlexNet (2012, SuperVision: Алекс Крижевский, Джеффри Хинтон и Илья Суцкевер)\n",
    "\n",
    "[ImageNet Classification with Deep Convolutional Neural Networks (Alex Krizhevsky, Geoffrey Hinton, Ilya Sutskever)](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
    "\n",
    "Долгое время на ILSVRC ошибка составляла порядка 25%, пока в 2012 году не вышла статья AlexNet. Эта сеть имеет схожую с LeNet-5 архитектуру, но имеет большую глубину и большее количество фильтров. И, конечно, использовалась для изображения из 3-х каналов. Использовалась активация ReLU и SGD с импульс. AlexNet обучался в течение 6 дней одновременно на двух графических процессорах Nvidia Geforce GTX 580.\n",
    "\n",
    "<img src=\"pictures/alex_net.png\" width=700 height=700 />\n",
    "\n",
    "### ZFNet (2013, Зейлер и Фергюс)\n",
    "\n",
    "[Visualizing and Understanding Convolutional Networks (Matthew Zeiler, Rob Fergus)](https://arxiv.org/pdf/1311.2901v3.pdf)\n",
    "\n",
    "Эта сеть является модификацией AlexNet.\n",
    "\n",
    "### VGG (2014, Симонян и Зиссерман)\n",
    "\n",
    "[Very Deep Convolutional Networks for Large-Scale Image Recognition (Karen Simonyan, Andrew Zisserman)](https://arxiv.org/pdf/1409.1556v6.pdf)\n",
    "\n",
    "Архитектура имеет 6 модификаций, наиболее популярная для извлечения признаков - **VGG-16**. Сеть тренировалась на 4 GPU в течение 2–3 недель. Смотри про **receptive field** ниже.\n",
    "\n",
    "<img src=\"pictures/vgg16.png\" width=700 height=700 />\n",
    "\n",
    "### GoogleLeNet (Inception) (2014)\n",
    "\n",
    "[Going Deeper with Convolutions](https://arxiv.org/pdf/1409.4842v1.pdf)\n",
    "\n",
    "В архитектуре данной сети впервые были использованы **inception блоки** (выделены овалами на изображении), что позволило добиться высокого качества работы модели при сравнительно (с VGG) небольшом количестве параметров.\n",
    "\n",
    "Еще одна особенность сети - две полносвязные подсети, которые имеют ту же функцию ошибки и делают промежуточные предсказания. Так мы следим, что градиент не затухнет при пробрасывании его через очень глубокую сеть на примере сетей \"поменьше\".\n",
    "\n",
    "<img src=\"pictures/googlenet.png\" width=600 height=600 />\n",
    "\n",
    "Также от создателей этой архитектуры есть статья [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/pdf/1512.00567.pdf), в которой они предлагают вместо свертки nxn использовать две последовательные свертки nx1 и 1xn.\n",
    "\n",
    "### ResNet (2015, Кайминг Хе)\n",
    "\n",
    "[Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "\n",
    "Архитектура, которая также называется **Residual Neural Network** (остаточное обучение). Ее особенностями являются сильная батч-нормализация и residual блоки, которые чем-то похожи на RNN. Есть разные модификации этой сети: ResNet-18, ResNet-32, ResNet-1024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Некоторые другие архитектуры CNN\n",
    "\n",
    "### U-Net\n",
    "\n",
    "[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)\n",
    "\n",
    "U-Net — это сверточная нейронная сеть, которая была представлена в 2015 году на соревновании ISBI для решения **задач сегментации** биомедицинских изображений. Специфика этой области подразумевает, что сеть должна показывать высокое качество даже при наличии небольшой обучающей выборки.\n",
    "\n",
    "Название U-Net хорошо отражает архитектуру сети, состоящую из сужающегося пути (contracting path) и расширяющегося пути (expansive path):\n",
    "\n",
    "<img src=\"pictures/unet2.png\" width=500 height=500 />\n",
    "\n",
    "**Contracting path** - типичная сверточная архитектура, состоящая из сверток 3×3 с активацией ReLU и MaxPooling 2×2. При этом с каждым шагом сети количество каналов изображения удваивается.\n",
    "\n",
    "**Expansive path**, наоборот, с каждым шагом уменьшает количество каналов изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Трюки, хаки, читы\n",
    "\n",
    "В этом разделе рассмотрим интересные идеи из различных CNN-архитектур.\n",
    "\n",
    "### Bottleneck (свертка 1x1)\n",
    "\n",
    "В чем смысл свертки 1x1? Есть несколько особенностей, способних привести к **ускорпению вычислений**:\n",
    "* Можно применить функцию активации и добавить нелинейность\n",
    "* Можно увеличить или уменьшить количество каналов, не меняя размер изображения (понижение размерности)\n",
    "* Можно выполнить какое-то линейное преобразование над изображением, например, умножить все пиксели на какое-то число\n",
    "\n",
    "### Receptive field\n",
    "\n",
    "Receptive field - \"поле внимания\", которое может обрабатывать некоторая архитектура нейонной сети. Чтобы выгодно менять это поле, можно применять несколько идущих подряд слоев свертки без слоев пулинга:\n",
    "\n",
    "<img src=\"pictures/receptive-field.png\" width=300 height=300 />\n",
    "\n",
    "Такой трюк применяется, например, в сети AlexNet или VGG, и помогает **уменьшить количество обучаемых параметров**. Рассмотрим 2 варианта:\n",
    "* Мы будем использовать свертку размера 7x7, тогда количество обучаемых параметров (без учета bias) = 49\n",
    "* Мы будем последовательно использовать 3 свертки 3x3, тогда количество обучаемых параметров (без учета bias) = 27\n",
    "\n",
    "Оба варианта имеют receptive field 7x7, при этом второй имеет **более глубокую архитектуру и меньшее количество параметров**.\n",
    "\n",
    "### Inception block\n",
    "\n",
    "Этот трюк впервые был внедрен в сети GoofLeNet. Inception block - это, по сути, сверточная нейронная сеть, состоящая из маленьких сверточных нейронных сетей, имеющих архитектуру, представленную на изображении:\n",
    "\n",
    "<img src=\"pictures/inception_module.png\" width=500 height=500 />\n",
    "\n",
    "Идея такой вложенной архитектуры - передача на вход не одной операции, а сразу нескольких (четырех, если быть точнее). Выходы после каждой свертки будут разными, так с одного изображения мы будем получать больше признаков.\n",
    "\n",
    "Вложенная архитектура состоит из следующих компонент:\n",
    "* Свертка 1x1\n",
    "* Свертка 3x3\n",
    "* Свертка 5x5\n",
    "* MaxPooling\n",
    "* И последующая конкатенация\n",
    "\n",
    "### Residual connections (ResNet)\n",
    "\n",
    "> Смотри также: **skip connections**\n",
    "\n",
    "[Wiki - Residual neural network](https://en.wikipedia.org/wiki/Residual_neural_network)\n",
    "\n",
    "**Residual block** - это блок нейронной сети, который на вход следующего за собой блока передает не только свой выход $F(x^{(k)})$, но и **свой вход** $x^{(k)}$ (остаток): \n",
    "\n",
    "$$x^{(k+1)} = F(x^{(k)}) + x^{(k)}$$\n",
    "\n",
    "<img src=\"pictures/res_con.png\" width=300 height=300 />\n",
    "\n",
    "Эта удивительно простая, похожая на RNN идея, **побеждает проблему затухания градиента** в глубоких нейронных сетях, так как теперь градиент никогда не равен нулю:\n",
    "\n",
    "$$\\frac{\\partial x^{(k+1)}}{\\partial x^{(k)}} = 1 + \\frac{\\partial F (x^{(k)})}{\\partial x^{(k)}}$$\n",
    "\n",
    "[Different skip connection schemes. (a) No skip connection. (b) Distinct-source skip connection. (c) Shared-source skip connection. (d) Denseskip connection](https://www.researchgate.net/publication/329750500_Multi-Memory_Convolutional_Neural_Network_for_Video_Super-Resolution)\n",
    "\n",
    "<img src=\"pictures/dif_res_con.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аугментации\n",
    "\n",
    "[Albumentations](https://demo.albumentations.ai/)\n",
    "\n",
    "Для большинства задач, связанных с обработкой изображений, можно искусственно расширять обучающую выборку за счет **аугментаций** данных - видоизменения имеющихся изображений:\n",
    "* Повороты\n",
    "* Отражения\n",
    "* Растяжения\n",
    "* Сжатия\n",
    "* Изменения цвета\n",
    "* Добавление шума\n",
    "* Зануление пикселей (шум соль и перец)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
