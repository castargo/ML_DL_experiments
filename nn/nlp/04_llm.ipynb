{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Model\n",
    "\n",
    "## Статьи\n",
    "\n",
    "| Год | Архитектура | Исследователи | Примечание |\n",
    "| --- | --- | --- | --- |\n",
    "| 2015 | [A Neural Conversational Model](https://arxiv.org/pdf/1506.05869.pdf) | Oriol Vinyals, Quoc Le |  |\n",
    "| 2015 | [Hierarchical Neural Network Generative Models for Movie Dialogues](https://www.researchgate.net/publication/280221106_Hierarchical_Neural_Network_Generative_Models_for_Movie_Dialogues) | Iulian Vlad Serban, Alessandro Sordoni, Y. Bengio, Aaron Courville | HRED - hierarchical recurrent encoder decoder neural network |\n",
    "| 2015 | [Attention with Intention for a Neural Network Conversation Model](https://arxiv.org/pdf/1510.08565.pdf) | Kaisheng Yao, Geoffrey Zweig, Baolin Peng |  |\n",
    "| 2017 | [Attention Is All You Need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) | Google Brain |  |\n",
    "\n",
    "## Литература\n",
    "* [Sequence to Sequence (seq2seq) and Attention by Lena Voita](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)\n",
    "* [NLP course by Hugging Face](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt)\n",
    "* [Transformer: A Novel Neural Network Architecture for Language Understanding](https://blog.research.google/2017/08/transformer-novel-neural-network.html)\n",
    "* [Attention? Attention! by Lilian Weng](https://lilianweng.github.io/posts/2018-06-24-attention/)\n",
    "* [Transformer в картинках](https://habr.com/ru/articles/486358/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура Transformer\n",
    "\n",
    "Трансформер — архитектура нейронных сетей, представленная в 2017 году исследователями из Google Brain в статье [Attention Is All You Need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf). Архитектура трансформера в статье выглядела следующим образом ([изображение взято из блога Лены Войты](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)):\n",
    "\n",
    "<img src=\"pictures/seq2seq_voita12.png\" width=800 height=800 />\n",
    "\n",
    "В этой архитектуре нет ни сверточных, ни рекуррентных компонентов. По аналогии с Encoder-Decoder архитектурой состовляющие Transformer-а можно разложить на следующие составляющие:\n",
    "* Encoder: Multi-Head Attention\n",
    "* Decoder: Masked Multi-Head Attention\n",
    "* Связь между encoder и decoder частями: Multi-Head Attention\n",
    "\n",
    "Эта архитектура стала SOTA-подходом для задачи машинного перевода, и сейчас де-факто является стандартом во многих задачах, связанных с NLP, CV и т.д. Также эта архитектура широко используется для различных языковых моделей, начало бурного развития которых и положила статья Attention Is All You Need.\n",
    "\n",
    "> Transformer = attention + pretraining + fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Языковые модели из Transformer\n",
    "\n",
    "* [Зоопарк моделей трансформеров на Hugging Face](https://huggingface.co/docs/transformers/model_doc/bert)\n",
    "* [Transformer models: an introduction and catalog — 2023 Edition](https://amatria.in/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/)\n",
    "    * [Catalog table](https://amatria.in/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/#catalog-table)\n",
    "    * [Family Tree](https://amatria.in/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/#family-tree)\n",
    "    * [Catalog List](https://amatria.in/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/#catalog-list)\n",
    "* [A Survey of Transformers](https://arxiv.org/pdf/2106.04554)\n",
    "* [Pre-trained Models for Natural Language Processing: A Survey](https://arxiv.org/abs/2003.08271)\n",
    "* [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyon](https://arxiv.org/pdf/2304.13712)\n",
    "    * The evolutionary tree of modern LLMs\n",
    "* [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223)\n",
    "* [Formal Algorithms for Transformers](https://arxiv.org/pdf/2207.09238)\n",
    "\n",
    "Кратко историю развития языковых моделей можно описать так:\n",
    "\n",
    "| Год | Архитектура | Статья | Исследователи | Примечание |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 2018-06 | **GPT-1** | [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) | OpenAI | 117 миллионов параметров, 5 GB обучающих данных |\n",
    "| 2018-10 | **BERT** | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805) | Google AI Language | [Страница BERT на HF](https://huggingface.co/docs/transformers/model_doc/bert) |\n",
    "| 2019-02 | **GPT-2** | [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | OpenAI | 1.5 миллиарда параметров, 40 GB обучающих данных |\n",
    "| 2019-10 | **DistilBERT** | [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/pdf/1910.01108) | Hugging Face | дистиллированный BERT, на 60% быстрее, на 40% легче по памяти, при этом составляет 97% производительности BERT |\n",
    "| 2019-10 | **BART** | [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461) | Facebook AI | [Страница BART на HF](https://huggingface.co/docs/transformers/model_doc/bart) |\n",
    "| 2019-10 | **T5** | [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683) | Google AI | [Страница T5 на HF](https://huggingface.co/docs/transformers/model_doc/t5) |\n",
    "| 2020-05 | **GPT-3** | [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf) | OpenAI | 175 миллионов параметров, 45000 GB обучающих данных |\n",
    "\n",
    "[**Хронологический таймлайн моделей и количества их параметров можно посмотреть здесь.**](https://amatria.in/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/#chronological-timeline)\n",
    "\n",
    "Зачастую общая стратегия достижения большей производительности языковых моделей (за исключением некоторых примеров, таких как DistilBERT), сводится к увеличинию количества параметров моделей, а также увеличению обучающей выборки.\n",
    "\n",
    "Глобально языковые модели из Transformer можно разделить на 3 категории:\n",
    "* **Автокодирующие модели (auto-encoding Transformer models)**: BERT-like\n",
    "    * Особенности:\n",
    "        * Использует только **энкодер** Трансформера\n",
    "        * Bi-directional attention, который \"видит\" все слова в предложении\n",
    "        * Предобучение таких моделей обычно строится на искажении исходного предложения (например, masked language modeling) и последующей попытке это исходное предложение восстановить\n",
    "        * Хороши для задач: классификации, NER, extractive question answering\n",
    "    * Примеры:\n",
    "        * BERT\n",
    "        * ALBERT\n",
    "        * DistilBERT\n",
    "        * ELECTRA\n",
    "        * RoBERTa\n",
    "* **Авторегрессионные модели (auto-regressive Transformer models)**: GPT-like\n",
    "    * Особенности:\n",
    "        * Использует только **декодер** Трансформера\n",
    "        * Attention декодера не может \"заглядывать в будущее\": Masked Self-Attention\n",
    "        * Предобучение таких моделей обычно строится на предсказании следующего слова в предложении\n",
    "        * Хороши для задач тектовой генерации\n",
    "    * Примеры:\n",
    "        * GPT: Generative Pre-trained Transformer\n",
    "        * CTRL\n",
    "        * Transformer XL\n",
    "* **Модели типа кодировщик-декодировщик (sequence-to-sequence Transformer models)**: BART/T5-like\n",
    "    * Особенности:\n",
    "        * Используются **энкодер и декодер** из Трансформера\n",
    "        * Стратегия предобучения таких моделей отличается от подхода к подходу\n",
    "        * Хороши для задач, связанных с созданием новых предложений в зависимости от заданных входных данных: суммаризация, перевод, generative question answering\n",
    "    * Примеры:\n",
    "        * BART: bidirectional and auto-regressive transformers\n",
    "        * mBART\n",
    "        * Marian\n",
    "        * T5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
