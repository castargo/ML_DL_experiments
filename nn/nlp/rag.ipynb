{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56736069",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "* [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)\n",
    "* FLARE: [Active Retrieval Augmented Generation](https://arxiv.org/abs/2305.06983)\n",
    "* [Архитектура RAG: полный гайд](https://habr.com/ru/amp/publications/791034/)\n",
    "* [RAG (Retrieval Augmented Generation) — простое и понятное объяснение](https://habr.com/ru/articles/779526/)\n",
    "* [RAG From Scratch](https://github.com/langchain-ai/rag-from-scratch?tab=readme-ov-file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f2b88c",
   "metadata": {},
   "source": [
    "## Компоненты RAG-системы\n",
    "\n",
    "RAG - это техника повышения точности и надежности LLM с использованием документов, полученных из внешних источников. \n",
    "\n",
    "Процесс генерации информации при помощи RAG выглядит следующим образом:\n",
    "1. Пользователь составляет **запрос**\n",
    "2. Запрос отправляется в **поисковик**\n",
    "    * Определяется процесс поиска и извлечения документов по запросу:\n",
    "        * **Retrievals**\n",
    "        * **Векторная база данных**\n",
    "3. Извлеченные документы отправляются в **генератор**\n",
    "    * Комбинация входного запроса и извлеченных документов\n",
    "4. Итоговый ответ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dbe32d",
   "metadata": {},
   "source": [
    "## Векторная база данных\n",
    "\n",
    "Векторные базы данных хранят векторы контекста (эмбеддинги). На основе такой базы знаний производится **семантический поиск**, который помогает определить, насколько документы релевантны запросу.\n",
    "\n",
    "1. Правильно нарезаем базу знаний на чанки для базы данных\n",
    "    * По символам? По абзацам? Логически? С перекрытием?\n",
    "    * Важные параметры: количество чанков, размер чанка\n",
    "2. Какой эмбедер выбрать\n",
    "    * Нужно ли его дообучать?\n",
    "    \n",
    "Примеры векторных баз:\n",
    "* FAISS\n",
    "* ChromaDB\n",
    "* QDrant\n",
    "* ElasticSearch\n",
    "* Weaviate\n",
    "* Milvus\n",
    "* Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1178b8",
   "metadata": {},
   "source": [
    "## Retrievals\n",
    "\n",
    "[Ретриверы из langchain_community](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/retrievers)\n",
    "\n",
    "* **Dense Retriever (embdedding similarity)** - используют трансформеры, например, BERT. Сходство между закодированными векторами вычисляется при помощи близости (cosine similarity).\n",
    "    * Стандартный ретривер - косинусная близость вектора запроса и вектора документа\n",
    "* **Sparse Retriever (разряженные вектора)** - традиционные методы информационного поиска, основанные на частотности\n",
    "    * TF‑IDF ретривер\n",
    "    * BM25 ретривер\n",
    "\n",
    "Можно использовать какой-то определенный ретривер, а можно построить **ансамбль ретриверов** при помощи EnsembleRetriever:\n",
    "* Результаты работы всех ретриверов объединяются\n",
    "* Общий пул документов ранжируется, например, при помощи, Reciprocal Rank Fusion\n",
    "\n",
    "> **MultiQueryRetriever**: перефразируем запрос несколько раз, извлечем документы по всем перефразированным вопросам, затем отранжируем общий пул документов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d038e41d",
   "metadata": {},
   "source": [
    "## Rank Fusion Techniques\n",
    "\n",
    "Вообще, очень важно в каком порядке LLM увидит результаты поиска. Чем выше наиболее релевантный документ, тем лучше будет финальная генерация. В случае использования одного ретривера выборка ранжируется по метрике этого ретривера. Если же ретриверов несколько, то нужно использовать **реранкер**.\n",
    "\n",
    "Внутри EnsembleRetriever используется Reciprocal Rank Fusion. Основная его идея - придать большее значение элементам, занимающим более высокие позиции в каждом индивидуальном наборе результатов поиска. Формула RRF: 1 / (k + rank), где rank - позиция элемента в конкретном поиске, а k - константа (по умолчанию k=60). Оценки для каждого элемента в разных наборах затем суммируются для получения итоговой оценки. Так мы избавляемся от привязки к метрике конкретного ретривера и подчеркиваем значимость наиболее высоко отранжированных элементов.\n",
    "\n",
    "Другие реранкеры:\n",
    "* Cohere Rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3831b7f7",
   "metadata": {},
   "source": [
    "## RAG-Fusion: Multi-Query Retrieval + Reranker (Rank Fusion Techniques)\n",
    "\n",
    "[RAG-Fusion: Multi-Query Retrieval & Rank Fusion Techniques](https://docsbot.ai/article/advanced-rag-techniques-multiquery-and-rank-fusion)\n",
    "\n",
    "Что будет, если вместо простой функции ранжирования документов использовать ML-модель? Идея похожа на описанную в предыдущем пункте:\n",
    "* Будем формулировать несколько вариантов запроса пользователя\n",
    "* Искать по каждому запросу документы в базе данных\n",
    "* Ранжировать и объединять результаты при помощи **Cross-Encoder**\n",
    "\n",
    "Вообще, векторные базы данных используют Bi-encoder-ы, чтобы вычислить похожесть двух векторов. На вход Bi-encoder принимает 1 документ и преобразует его в вектор. Тогда схема работы RAG-системы с Bi-encoder выглядит так:\n",
    "```\n",
    "Текст А --> BERT преобразование в вектор --> косинусная мера близости <-- BERT преобразование в вектор <-- Текст Б\n",
    "```\n",
    "\n",
    "Cross-Encoder принимает на вход 2 документа и возвращает их релевантность (similarity) относительно друг друга. Схема работы Cross-Encoder выглядит иначе:\n",
    "```\n",
    "Текст А, Текст Б --> BERT --> Классификатор --> Релевантность 0..1\n",
    "```\n",
    "\n",
    "Точность работы Cross-Encoder, как правило, выше, чем у Bi-encoder. Идея заключается в том, чтобы извлечь из базы побольше результатов, а затем отранжировать их при помощи Cross-Encoder и вернуть на вход LLM 3-5 из них.\n",
    "\n",
    "> **Интент-классификатор**: можно классифицировать документы на вопросы, жалобы, просьбы и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d85c1a3",
   "metadata": {},
   "source": [
    "## Дообучение LLM\n",
    "\n",
    "В задаче RAG на вход LLM поступают пары сущностей: входной запрос и контекст, состоящий из документов. LLM можно натюнить на такой вход."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4f899e",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Language Model based Prediction\n",
    "\n",
    "Будем на шаге поиска вместо обычно передачи результатов на генерацию, генерировать примеры ответов для LLM (few-shot prompting), заставляя ее динамически обучаться отвечать на поставленный вопрос."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa2efc2",
   "metadata": {},
   "source": [
    "## Промпт-инжениринг\n",
    "\n",
    "#### Корректный системный промпт\n",
    "\n",
    "Источник: [GigaChain работа с промптами](https://github.com/ai-forever/gigachain/tree/master/hub/prompts)\n",
    "\n",
    "```\n",
    "input_variables: []\n",
    "output_parser: null\n",
    "template: 'Ты - система информационного поиска. Тебе дан вопрос и релевантные отрывки текста из нескольких документов.\n",
    "Создай краткий и информативный ответ (не более 150 слов) на заданный вопрос,\n",
    "основываясь исключительно на приведенных отрывках документов. Ты должна использовать только информацию из приведенных отрывков.\n",
    "Используй непредвзятый и журналистский тон. Не повторяй текст.\n",
    "Создай окончательный ответ (\"FINAL ANSWER\").\n",
    "Не пытайся придумать ответ.\n",
    "Отвечай только на русском языке за исключением специальных терминов.\n",
    "Если документы не содержат ответа на вопрос, скажи, что \"Я не могу ответить на вопрос на основе информации. Попробуйте переформулировать вопрос.\"\n",
    "template_format: f-string\n",
    "_type: prompt\n",
    "```\n",
    "\n",
    "#### Пользовательский промпт\n",
    "\n",
    "```\n",
    "input_variables: [question, summaries]\n",
    "output_parser: null\n",
    "template: \"QUESTION: {question}\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "FINAL ANSWER:\"\n",
    "template_format: f-string\n",
    "_type: prompt\n",
    "```\n",
    "\n",
    "#### Промпт для обработки первоначального запроса пользователя\n",
    "\n",
    "```\n",
    "Вот запрос пользователя. Думай по шагам и оцени, что пользователь хотел узнать.\n",
    "В качестве ответа выведи настоящий запрос, который хотел ввести пользователь.\n",
    "```\n",
    "\n",
    "#### Промпт для суммаризации документа\n",
    "\n",
    "Если по запросу найдено много длинных чанков, можно попосить LLM суммаризовать каждый из нейденных чанков.\n",
    "\n",
    "> ConversationSummaryMemory\n",
    "\n",
    "#### Промпт для ранжирования результатов\n",
    "\n",
    "Вместо использования реранкера можно попросить LLM отранжировать документы в контексте.\n",
    "\n",
    "#### Промпт для реврайта\n",
    "\n",
    "На выходе можно попросить LLM вывести сгенерированный результат в определенном формате."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104112b",
   "metadata": {},
   "source": [
    "## Метрики оценки RAG-системы\n",
    "\n",
    "#### Формируем выборку для оценки\n",
    "\n",
    "**Golden set** (вопросы и ответы) должен быть составлен при помощи естественного интеллекта. Лучше всего, если будет несколько экземпляров ответов на один вопрос.\n",
    "\n",
    "#### Метрики поиска (ранжирования)\n",
    "\n",
    "Не учитывающие порядок:\n",
    "* Precision@K (P@K)\n",
    "\n",
    "Учитывающие порядок:\n",
    "* MAP@K\n",
    "* NDCG@K\n",
    "\n",
    "#### Метрики генерации\n",
    "\n",
    "ROUGE, BERTScore, BLEURT, METEOR. Можно написать взвешенную сумму перечисленных метрик.\n",
    "\n",
    "А еще можно попросить LLM вернуть не сгенерированные токены, а их логиты, и оценить, насколько она сама уверена в ответе (token level uncertainty)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175a050b",
   "metadata": {},
   "source": [
    "## Проблемы в RAG-системах и способы их решения\n",
    "\n",
    "* [12 RAG Pain Points and Solutions](https://originshq.com/blog/12-rag-pain-points-and-solutions/)\n",
    "* [Seven Failure Points When Engineering a Retrieval Augmented Generation System](https://arxiv.org/pdf/2401.05856)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
