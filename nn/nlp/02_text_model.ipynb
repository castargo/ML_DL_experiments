{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модели текста и способы представления слов (word embeddings)\n",
    "\n",
    "* \"Глубокое обучение. Погружение в мир нейронных сетей\" - Николенко, Кадурин, Архангельская\n",
    "* [Word Embeddings by Lena Voita](https://lena-voita.github.io/nlp_course/word_embeddings.html)\n",
    "* [Модель текста](https://ped_recheved.academic.ru/117/%D0%9C%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0)\n",
    "* [Автоматическая обработка текстов на естественном языке и анализ данных - Большакова, Воронцов, Ефремова, Клышинский, Лукашевич, Сапин](https://www.hse.ru/data/2017/08/12/1174382135/NLP_and_DA.pdf)\n",
    "\n",
    "Модель текста - освобожденное от несущественных для целей моделирования представление содержания и формы текста (например, в опорных словах, словосочетаниях и предложениях), которое позволяет отобразить жанр, тему, композицию, структуру и способ развертывания содержания.\n",
    "\n",
    "Для части задач компьютерной лингвистики не требуется полного понимания смысла текста, достаточно правильно описать его содержание. Модели текста позволяют сравнивать тексты друг с другом и единообразно обрабатывать их коллекции.\n",
    "\n",
    "При этом можно выделить две группы подходов к построению моделей текста:\n",
    "* Подходы, в которых не учитываются связи между словами (частотные или признаковые модели текста)\n",
    "* Подходы, в которых связи учитываются (**дистрибутивная семантика**: информация о контекстах слов помещается в их векторное представление):\n",
    "    * \"Классические\" статистические подходы: взаимная встречаемость слов зависит от глобальной статистики корпуса\n",
    "    * Нейросетевые предиктивные подходы\n",
    "    \n",
    "Тогда мы можем представить слова как вектора вещественных чисел $R^d$: геометрические соотношения в пространстве $R^d$ будут соответствовать семантическим соотношениям между словами. Близкие векторы - слова-синонимы. \n",
    "\n",
    "**Дистрибутивная гипотеза**: лингвистические единицы, встречающиеся в схожих контекстах, имеют близкие значения. [Отличный пример от Лены Войты.](https://lena-voita.github.io/nlp_course/word_embeddings.html)\n",
    "\n",
    "Рассмотрим различные примеры моделей текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Признаковая модель текста\n",
    "\n",
    "Признаковая модель рассматривает текст как неупорядоченный набор признаков:\n",
    "\n",
    "* Лингвистические признаки\n",
    "    * Слова\n",
    "    * N-граммы\n",
    "* Статистические признаки\n",
    "    * Кол-во восклицательных знаков\n",
    "    * Доля различных частей речи\n",
    "* Экстралингвистические признаки\n",
    "    * Заголовок документа\n",
    "    * Дата публикации\n",
    "    * Гиперссылки\n",
    "\n",
    "Рассмотрим поподробнее модели, строящиеся на лингвистических признаках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words (мешок слов)\n",
    "\n",
    "Простейший пример текстовой модели можно реализовать при помощи **словаря разрешенных слов**, в котором все известные нам слова закодированы уникальными индексами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {0: 'UNK', 1: 'i', 2: 'saw', 3: 'a', 4: 'cat'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть теперь у нас имеется $N$ текстов, мы можем представить каждый текст в виде вектора (**one-hot vectors**): $w_j = (w_{j1}, ..., w_{jM})$, где $w_{ji}$ - вес i-го слова в j-ом тексте, $M$ - размер словаря.\n",
    "\n",
    "Таким образом, **мешок слов** - это модель представления текста в виде вектора (неупорядоченного набора слов). Каждому слову в тексте сопоставляется число его вхождений.\n",
    "\n",
    "[Пример](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "        [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "        [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 0, 0, 1, 0, 1]]),\n",
       " ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X.toarray(), vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Недостатки подхода:\n",
    "* Не учитываются связи между словами и не учитывается порядок слов в тексте\n",
    "    * Банк купил компанию и Компания купила банк - это одинаковые вектора\n",
    "* Стоп-слова (предлоги, союзы и прочее) будут наиболее частотными в каждом документе, их стоит удалять\n",
    "* Для больших словарей вектора будут очень длинными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-граммная модель текста\n",
    "\n",
    "Данный подход похож на bag-of-words с той лишь разницей, что используются последовательности слов длины N. Таким образом, можно сказать, что мы учитываем порядок слов.\n",
    "\n",
    "В данном подходе можно использовать не только слова, но и **буквенные N-граммы**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
       "        [1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]]),\n",
       " ['and this',\n",
       "  'document is',\n",
       "  'first document',\n",
       "  'is the',\n",
       "  'is this',\n",
       "  'second document',\n",
       "  'the first',\n",
       "  'the second',\n",
       "  'the third',\n",
       "  'third one',\n",
       "  'this document',\n",
       "  'this is',\n",
       "  'this the'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X.toarray(), vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-граммы\n",
    "\n",
    "k-skip-n-граммы - наборы из n токенов, причем расстояние между соседними должно составлять не более k токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Insurgents killed in ongoing fighting\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Insurgents', 'killed'),\n",
       " ('Insurgents', 'in'),\n",
       " ('Insurgents', 'ongoing'),\n",
       " ('killed', 'in'),\n",
       " ('killed', 'ongoing'),\n",
       " ('killed', 'fighting'),\n",
       " ('in', 'ongoing'),\n",
       " ('in', 'fighting'),\n",
       " ('ongoing', 'fighting')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(skipgrams(sent, 2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Insurgents', 'killed', 'in'),\n",
       " ('Insurgents', 'killed', 'ongoing'),\n",
       " ('Insurgents', 'killed', 'fighting'),\n",
       " ('Insurgents', 'in', 'ongoing'),\n",
       " ('Insurgents', 'in', 'fighting'),\n",
       " ('Insurgents', 'ongoing', 'fighting'),\n",
       " ('killed', 'in', 'ongoing'),\n",
       " ('killed', 'in', 'fighting'),\n",
       " ('killed', 'ongoing', 'fighting'),\n",
       " ('in', 'ongoing', 'fighting')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(skipgrams(sent, 3, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Статистическая модель текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive Pointwise Mutual Information (PPMI)\n",
    "\n",
    "Точечная взаимная информация (PMI) помогает ответить на вопрос: происходят ли события x и y совместно, чем по отдельности? Эту идею легко переложить на концепцию совстречаемости слов (Church & Hanks 1989) и рассчитать матрицу PPMI:\n",
    "\n",
    "$$\\text{PMI}(w_1, w_2) = \\log_2{(\\frac{P(w_1, w_2)}{P(w_1)P(w_2)})}$$\n",
    "\n",
    "$$\\text{PPMI}(w_1, w_2) = \\text{max}(0, \\text{PMI}(w_1, w_2))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Матрица совместной встречаемости\n",
    "\n",
    "Выше уже были предложены различные методы для формирования матрицы совместной встречаемости $ X \\in R^{V \\times V}$, где $V$ - набор токенов (слов, N-грамм и т.д.).\n",
    "\n",
    "Один из наибольших недостатков матрицы совместной встречаемости - сильная разряженность.\n",
    "\n",
    "Для эффективного хранения такой большой структуры можно использовать **[сингулярное разложение матриц (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition)**: трансформируем матрицу в вектора.\n",
    "\n",
    "> [Latent semantic analysis (LSA)](https://en.wikipedia.org/wiki/Latent_semantic_analysis)\n",
    "\n",
    "LSA тесно связано с тематическим моделированием (pLSA), которое является его вероятностной интерпретацией.\n",
    "\n",
    "Латентное размещение Дирихле (LDA) - байесовский вариант pLSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (задание весов признакам)\n",
    "\n",
    "**TF (term frequency — частота слова)** — отношение числа вхождений некоторого слова к общему числу слов документа. Таким образом, оценивается важность слова $t_i$ в пределах отдельного документа:\n",
    "\n",
    "$$\\text{tf}(t, d) = \\frac{n_t}{\\sum\\limits_{k} n_k}$$\n",
    "\n",
    "где \n",
    "* $n_t$ - число вхождений слова $t$ в документ \n",
    "* $\\sum\\limits_{k} n_k$ - число слов в документе\n",
    "\n",
    "**IDF (inverse document frequency — обратная частота документа)** — инверсия частоты, с которой некоторое слово встречается в документах коллекции. Учёт IDF уменьшает вес широкоупотребительных слов. Для каждого уникального слова в пределах конкретной коллекции документов существует только одно значение IDF.\n",
    "\n",
    "$$\\text{idf}(t, D) = log(\\frac{|D|}{|\\{ d_i \\in D | t \\in d_i \\}|})$$\n",
    "\n",
    "где \n",
    "* $|D|$ - число документов в коллекции\n",
    "* $|\\{ d_i \\in D | t \\in d_i \\}|$ - число документов в коллекции $D$, в которых встречается $t$ (когда $n_t \\neq 0$)\n",
    "\n",
    "Выбор основания логарифма в формуле не имеет значения, поскольку изменение основания приводит к изменению веса каждого слова на постоянный множитель, что не влияет на соотношение весов. Логарифм используется, чтобы «смягчить» разницу в употреблении более частотных и менее частотных слова.\n",
    "\n",
    "Таким образом, мера TF-IDF является произведением двух сомножителей:\n",
    "\n",
    "$$\\text{tf-idf}(t, d, D) = \\text{tf}(t, d) \\times \\text{idf}(t, D)$$\n",
    "\n",
    "Большой вес в TF-IDF получат слова с высокой **частотой** в пределах конкретного документа и с низкой частотой употреблений в других документах.\n",
    "\n",
    "Документы как вектора: \n",
    "* $|V|$ - мерное векторное пространство\n",
    "* Слова – это отдельные позиции (оси) в векторах\n",
    "* Документы – точки или вектора в пространстве\n",
    "* Очень большой размерности: десятки миллионов  для интернета\n",
    "* Вектора документов – разреженные\n",
    "\n",
    "Формализация близости векторов:\n",
    "* Вектора запроса и документов – точки в многомерном пространстве\n",
    "* Как найти близость между векторами? Предположение: чем меньше угол, тем более похожи вектора. Косинусная мера: чем больше косинус  угла между векторами, тем более похожи документы\n",
    "\n",
    "$$\\cos(\\vec q, \\vec d) = \\frac{\\vec q \\cdot \\vec d}{|q| |d|} = \\frac{\\sum q_i d_i}{\\sqrt{\\sum q_i^2} \\sqrt{\\sum d_i^2}}$$\n",
    "\n",
    "где\n",
    "* $q_i$ - tf-idf вес слова i в запросе\n",
    "* $d_i$ - tf-idf вес терма i в документе\n",
    "* $q$ может быть не только запросом, но и другим документом\n",
    "\n",
    "$\\cos(\\vec q, \\vec d)$ - это косинусное сходство между $\\vec q$ и $\\vec d$, что эквивалентно косинусу угла между $\\vec q$ и $\\vec d$.\n",
    "\n",
    "> [Tf-Idf and Cosine similarity](https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/)\n",
    "\n",
    "Варианты весов\n",
    "* Базовый – это сколько раз слово встретилось в документе (count), \n",
    "* Логарифмирование count\n",
    "* Нормализация первого сомножителя (tf/tfmax или tf/|d|, где |d| - количество слов в документе)\n",
    "\n",
    "TF-IDF опирается на предположение: **при обработке коллекции текстов частотные признаки менее информативны, чем редкие. Веса частотных признаков должны быть ниже, чем веса редких.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 9),\n",
       " ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X.shape, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предиктивная модель текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Word2Vec (2013)](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "* [Word2Vec - wiki](https://en.wikipedia.org/wiki/Word2vec)\n",
    "* [Learning Word Embedding](https://lilianweng.github.io/posts/2017-10-15-word-embedding/)\n",
    "* [word2vec Parameter Learning Explained](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "\n",
    "Идея word embeddings заключается в том, чтобы каждому слову из словаря $i \\in V$ сопоставить вектор $w_i \\in R^d$, где $d$ - это достаточно большое значение, но все еще меньшее размера словаря (наша цель - довольно небольшая, но плотная матрица представлений). \n",
    "\n",
    "Вероятность того, что слово $i$ встречается в данном контексте (то есть вместе со словами $c_1, ..., c_n$): \n",
    "\n",
    "$$\\hat p (i | c_1, ..., c_n) = f(w_i, w_{c_1}, ..., w_{c_n}; \\theta)$$\n",
    "\n",
    "Где $w_{c_1}, ..., w_{c_n}$ - это векторы слов контекста, $f$ - функция с обучаемыми параметрами $\\theta$. Функция... Или нейронная сеть.\n",
    "\n",
    "Будем обучать такую нейронную сеть на некотором корпусе/коллекции текстов, оптимизируя логарифм общего правдоподобия корпуса:\n",
    "\n",
    "$$L(W, \\theta) = \\frac{1}{T} \\sum_t \\log{f(w_t, w_{t-1}, ..., w_{t-n+1}; \\theta)}$$\n",
    "\n",
    "где $T$ - количество возможных окон слов (3-10 окруждающих слов).\n",
    "\n",
    "Какая архитектура может быть у такой нейронной сети? В статье [Word2Vec](https://arxiv.org/pdf/1301.3781.pdf) исследователь Миколов предложил сразу 2 вида:\n",
    "\n",
    "<img src=\"pictures/word2vec3.png\" width=700 height=700 />\n",
    "\n",
    "#### **CBOW (Continuous Bag-of-Words)**\n",
    "* Предсказывает слово при заданном контексте (мешке слов). То есть мы знаем окружение слова справа и слева в некотором заданном окне, нужно непосредственно предсказать это слово.\n",
    "* Архитектура - двухслойная нейронная сеть-кодировщик прямого распространения, которая достаточно быстро работает для больших корпусов. В упрощенном виде ее можно записать так:\n",
    "\n",
    "```python\n",
    "class CBOW(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size=128):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        # Слой 1\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_size)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        \n",
    "        # Слой 2: i-ая строка - представление i-го слова из словаря\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        self.activation_function2 = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Сумма или усреднее входных эмбеддингов (контекста)\n",
    "        embeds = sum(self.embeddings(inputs)).view(1, -1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "```\n",
    "\n",
    "<img src=\"pictures/word2vec-cbow.png\" width=500 height=500 />\n",
    "\n",
    "#### **SkipGrams (Skip-n-Grams)**\n",
    "* Предсказывает контекст при заданном слове, где контекст - это N-граммы\n",
    "* Архитектура - также двухслойная нейронная, работающая медленне, чем для CBOW (эффективнее для поиска редких слов в небольших корпусах).\n",
    "\n",
    "Подробнее с математическими аспектами обучения модели Word2Vec можно ознакомиться в статье: [word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method](https://arxiv.org/pdf/1402.3722.pdf). Наиболее важными моментами являются:\n",
    "* Для одного и того же слова используется 2 вектора: один - когда слово является центральным словом в окне (c - central), а второй - когда слово является одним из слов контекста (o - outside). \n",
    "    * Cлово может встретиться в своем же контексте! \n",
    "* Таким образом, мы **оптимизируем 2 матрицы параметров**, a соответствующая softmax-функция будет выглядеть следующим образом:\n",
    "\n",
    "$$p(o|c) = \\frac{\\text{exp}(u_o^T v_c)}{\\sum_{w \\in V} \\text{exp}(u_w^T v_c)}$$\n",
    "\n",
    "* Таким образом, при вычислении градиента мы будем считать его для пары слов (текущее центральное слово, текущее слово контекста). Грубо говоря, мы бежим в цикле по словам из словаря и проверям, встречается ли слово контекста вместе с ним в некоторм окне. Тогда лосс-функция будет иметь вид:\n",
    "\n",
    "$$-u_o^T v_c + \\text{log}(\\sum_{w \\in V} \\text{exp}(u_w^T v_c))$$\n",
    "\n",
    "* **Negative sampling**\n",
    "    * $\\sum_{w \\in V} \\text{exp}(u_w^T v_c)$ - довольно сложная операция, нужно за один шаг обновить $|V| + 1$ параметр. Вместо того, чтобы считать полную сумму и обновлять все параметры, будем обновлять выборочно лишь некоторые из них. Так мы будем оптимизировать правдоподобие немного другого события, но это все еще будет работать.\n",
    "    * Также в этом трюке мы переходим от softmax к сигмоиде: так как мы считаем градиент для пары слов, то можем поставить задачу **присутствия пары слов в словаре**, то есть бинарную классификацию. Сейчас у нас все пары из сканирующего окна в словаре есть, поэтому нужно насемплировать пары слов, которые в одном контексте не встречаютсе - нулевой класс.\n",
    "    \n",
    "> Можно показать, что Word2Vec неявно аппроксимируют факторизацию (сдвинутой) матрицы PMI (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем работать с [предобученным word2vec](https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300?select=GoogleNews-vectors-negative300.bin):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'GoogleNews-vectors-negative300.bin',\n",
    "    binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = model.index_to_key\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cute', 0.7902224063873291),\n",
       " ('cutest', 0.648817777633667),\n",
       " ('lovable', 0.6460723876953125),\n",
       " ('adorable_puppy', 0.6357036828994751),\n",
       " ('loveable', 0.6204661726951599),\n",
       " ('sooo_cute', 0.6165933012962341),\n",
       " ('adorably', 0.6149011850357056),\n",
       " ('vulnerable_Frangipane', 0.6049544811248779),\n",
       " ('Adorable', 0.6045286059379578),\n",
       " ('endearing', 0.5991296172142029)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('adorable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или можем обучить (дообучить) свою модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(vector_size=300, window=7, min_count=10, workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(corpus, total_examples=len(corpus), epochs=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [GloVe (Global Vectors - 2014)](https://aclanthology.org/D14-1162.pdf)\n",
    "\n",
    "* [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "GloVe - это объединение статистической и предиктивной модели: будем обучать градиентным спуском **глобальные статистики** корпуса текстов. \n",
    "\n",
    "Глобальные статистики до этого мы упомянали, когда говорили про матрицу совместной встречаемости слов: $X \\in R^{V \\times V}$. \n",
    "\n",
    "|  | мама | мыла | раму |\n",
    "| --- | --- | --- | --- |\n",
    "| мама | 0 | 6 | 0 |\n",
    "| мыла | 0 | 1 | 3 |\n",
    "| раму | 1 | 2 | 0 |\n",
    "\n",
    "В такой матрице общее число совместных встречаемостей слова $i$: $X_i = \\sum_k X_{ik}$, а вероятность встретить слово $j$ в контексте слова $i$:\n",
    "\n",
    "$$p(j|i) = \\frac{X_{ij}}{\\sum_k X_{ik}}$$\n",
    "\n",
    "Такие вероятности достаточно трудно интерпретировать в отличае от **отношений вероятностей**: чем больше значение соотношения, тем чаще одно слово встречается в контексте другого. Так GloVe обучает нейронную сеть, аппрксимирующую отношения вероятностей слов из глобального корпуса:\n",
    "\n",
    "$$J(\\theta) = \\sum_{i, j = 1}^{|V|} f(X_{ij}) (w_i^T \\tilde w_j + b_i + \\tilde b_j - \\log{X_{ij}})^2$$\n",
    "\n",
    "где  \n",
    "* $w_i$ - векторное представление слова, а $\\tilde w_j$ - векторное представление контекста\n",
    "* $b_i$ и $\\tilde b_j$ - соответствующие векторы сдвига (bias)\n",
    "* $f(x)$ - весовая функция, где $\\alpha = 0.75$, а $x_{max} = 100$:\n",
    "$$f(x)={\\begin{cases}(\\frac{x}{x_{max}})^\\alpha,&\\text{if } x < x_{max};\\\\1,& \\text{otherwise}\\end{cases}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 0.6603151559829712),\n",
       " ('sister', 0.6491517424583435),\n",
       " ('cute', 0.6427398920059204),\n",
       " ('omg', 0.6382595300674438),\n",
       " ('friend', 0.6285562515258789),\n",
       " ('girl', 0.6278365850448608),\n",
       " ('animal', 0.6256914734840393),\n",
       " ('dad', 0.6187349557876587),\n",
       " ('picture', 0.6186913847923279),\n",
       " ('pet', 0.618313193321228)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"cat\", \"adorable\"], negative=[\"fluffy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [FastText (2017)](https://aclanthology.org/Q17-1010.pdf)\n",
    "\n",
    "* [Library for efficient text classification and representation learning](https://fasttext.cc/)\n",
    "\n",
    "Особенности:\n",
    "* Negative sampling\n",
    "* Эмбеддинги N-грамм\n",
    "* Subword-embeddings модель: разбиение токена на символьные N-граммы по 3-6 символов:\n",
    "    * Пример разбиения слова \"проект\" на буквенные триграммы: {'#пр', 'про', 'рое', 'оек', 'ект', 'кт#'}\n",
    "    * Словарь из таких триграмм гораздо короче словаря всех слов\n",
    "* Хэширование таблицы признаков\n",
    "* Архитектура похожа на CBOW Word2Vec\n",
    "\n",
    "> Deep Structured Semantic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-Pair Encoding (BPE)\n",
    "\n",
    "* [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf)\n",
    "* [BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages](https://aclanthology.org/L18-1473.pdf)\n",
    "* [Сайт проекта](https://bpemb.h-its.org/)\n",
    "* [Репозиторий проекта](https://github.com/bheinzerling/bpemb)\n",
    "* [Hugging face NLP course - Byte-Pair Encoding tokenization](https://huggingface.co/learn/nlp-course/en/chapter6/5)\n",
    "* [Let's build the GPT Tokenizer - Andrej Karpathy](https://www.youtube.com/watch?v=zduSFxRajkE&ab_channel=AndrejKarpathy) + [repo - minbpe](https://github.com/karpathy/minbpe)\n",
    "* [Токенизация на подслова (Subword Tokenization) - Александр Дьяконов](https://alexanderdyakonov.wordpress.com/2019/11/29/%D1%82%D0%BE%D0%BA%D0%B5%D0%BD%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F-%D0%BD%D0%B0-%D0%BF%D0%BE%D0%B4%D1%81%D0%BB%D0%BE%D0%B2%D0%B0-subword-tokenization/)\n",
    "* [tiktoken (a fast BPE tokeniser for use with OpenAI's models)](https://github.com/openai/tiktoken)\n",
    "\n",
    "Впервые подход кодирования на подслова был предложен еще в 1994 (by Philip Gage). Суть подхода заключается в том, что в качестве токена будут использоваться не слова, а наборы символов с некоторой глубиной кодирования. Подход позволяет сжать последовательности за счет увеличения словаря токенов.\n",
    "\n",
    "При помощи BPE-tokenizer можно решить несколько проблем, присутствующих у таких моделей как Word2Vec, Glove и т.п.:\n",
    "* Независимость векторов для разных версий одного и того же слова\n",
    "* Кодирование отсутствующих в словаре слов\n",
    "* Большой размер словаря для больших корпусов текстов\n",
    "\n",
    "Обучение BPE-алгоритма:\n",
    "1. **Словарь токенов** инициализируется при помощи списка всех возможных символов, из которых состоят слова корпуса (как минимум, это все символы ASCII + некоторые символы Unicode)\n",
    "2. Затем до достижения сходимости ищем в корпусе пары токенов, которые встречаются наиболее часто:\n",
    "    * Инициализируем пустую таблицу (**merge table**)\n",
    "    * Производим подсчет того, как часто пары символов встречаются в корпусе\n",
    "    * Наиболее часто встречаемая пара символов добавляется в словарь и далее интерпретируется как **единый токен**\n",
    "\n",
    "При инференсе BPE-алгоритма выученные правила применяются для сегментации слов:\n",
    "* Каждое слово разбивается на последовательность символов\n",
    "* Среди всех последовательностей символов ищем в merge table самую длинную и выполняем слияние\n",
    "\n",
    "Таблица слияний упорядочена: слияния, находящиеся выше в таблице, чаще встречались в данных. Поэтому в алгоритме более высокие слияния имеют более высокий приоритет: на каждом шаге мы объединяем наиболее частое слияние среди всех возможных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизаторы из LLM\n",
    "* Токенизаторы GPT-2 и RoBERTa используют **byte-level BPE**: они рассматривают слова как байты (256 символов)\n",
    "* В BERT используется **WordPiece**\n",
    "* Другие токенизаторы:\n",
    "    * **Unigram Language Model (ULM)**\n",
    "    * **SentencePiece**\n",
    "    * **BPE-Dropout**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BACKLOG\n",
    "* WordNet и Freebase\n",
    "* RC-NET\n",
    "* Doc2Vec\n",
    "* PV-DM и PV-DBOW"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
