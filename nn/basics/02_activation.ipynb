{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции активации\n",
    "\n",
    "Свойства функции активации:\n",
    "* Она ограничена\n",
    "* Она непрерывна\n",
    "* Она монотонна\n",
    "* Она везде дифференцируема (но не всегда)\n",
    "\n",
    "### Пороговая функция активации (функция Хевисайда)\n",
    "* Бинарная классификация\n",
    "* Использовалась в нейроне МакКаллока-Питтса\n",
    "* Не определена в точке 0\n",
    "* $f'(x) = 0$\n",
    "\n",
    "$$\\varphi (x)={\\begin{cases}1,&x> 0;\\\\0,&x<0.\\end{cases}}$$\n",
    "\n",
    "### Функция знака\n",
    "* Бинарная классификация\n",
    "* Использовалась в перцептроне Розенблата\n",
    "\n",
    "$$\\varphi(x) = sign(x)$$\n",
    "\n",
    "### Линейная функция активации\n",
    "* Всегда есть ненулевой градиент\n",
    "$$\\varphi(x) = Cx$$\n",
    "\n",
    "### Логистическая функция активации (сигмоида)\n",
    "$$\\varphi(x) = \\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "* $f'(x) = f(x)(1 - f(x))$\n",
    "* $0 < \\sigma(x) < 1$:\n",
    "    * При $x \\rightarrow \\infty, \\quad \\sigma(x) \\rightarrow 1$\n",
    "    * При $x \\rightarrow -\\infty, \\quad \\sigma(x) \\rightarrow 0$\n",
    "* Не центрирована относительно 0, что снижает скорость сходимости\n",
    "* Быстро насыщается при очень больших и очень маленьких значениях аргумента, что плохо, так как в таком случае градиент функции становится близким к нулю, что приводит к затуханию градиента\n",
    "* Может быть интерпретирована как вероятность или log-вероятность принадлежности объекта к классу 1\n",
    "\n",
    "### Гиперболический тангенс\n",
    "$$\\varphi(x) = tanh(x) = {\\frac {e^{x}-e^{-x}}{e^{x}+e^{-x}}} = {\\frac {e^{2x}-1}{e^{2x}+1}}$$\n",
    "\n",
    "* $f'(x) = 1 - f^2(x)$\n",
    "* $-1 < tanh(x) < 1$\n",
    "* Центрирован в 0\n",
    "* Быстро насыщается\n",
    "\n",
    "### Softsign\n",
    "$$\\varphi(x) = softsign(x) = \\frac{x}{1 + |x|}$$\n",
    "* $f'(x) = \\frac{1}{(1 + |x|)^2}$\n",
    "* Центрирован в 0\n",
    "* Быстро насыщается\n",
    "\n",
    "### ReLU (Rectified linear unit) или линейный выпрямитель\n",
    "$$\\varphi(x) = ReLU(x) = max\\{0, x\\}$$\n",
    "* Кусочно-линейная функция активации\n",
    "* Ускоряет вычисления засчет простоты:\n",
    "$$f'(x) = \\begin{equation*}\\begin{cases} 0, & x < 0 \\\\ 1, & x > 0 \\end{cases} \\end{equation*}$$\n",
    "* Не дифференцируема в нуле - в этой точке необходимо использовать правую или левую производную\n",
    "* При неудачной инициализации весов может всегда \"гасить\" нейрон в ноль\n",
    " \n",
    "### Модификации ReLU\n",
    "* SoftPlus: $$\\log{(1 + e^x)}$$\n",
    "* Leaky ReLU (LReLU):\n",
    "$$\n",
    "\\begin{equation*}\n",
    "LReLU(x) =  \n",
    " \\begin{cases}\n",
    "   ax, & x < 0\\\\\n",
    "   x, & x > 0\n",
    " \\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n",
    "* Parametric ReLU (PReLU): аналогично с LReLU, но константа обучается как гиперпараметр\n",
    "* Exponential Linear Unit (ELU):\n",
    "$$\n",
    "\\begin{equation*}\n",
    "PLU(x) =  \n",
    " \\begin{cases}\n",
    "   \\alpha (e^x - 1), & x < 0\\\\\n",
    "   x, & x \\ge 0\n",
    " \\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "### Графики функций активации\n",
    "<img src=\"pictures/activations.png\" width=500 height=500 />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
