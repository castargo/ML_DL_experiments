{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции потерь (функции ошибки)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Литература\n",
    "\n",
    "1. [\"Нейронные сети и компьютерное зрение\" - Samsung Russia Open Education](https://stepik.org/lesson/241448/step/2?auth=login&unit=213792)\n",
    "2. \"Глубокое обучение. Погружение в мир нейронных сетей\" - Николенко, Кадурин, Архангельская\n",
    "3. [Wiki - Метод максимального правдоподобия](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BC%D0%B0%D0%BA%D1%81%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B3%D0%BE_%D0%BF%D1%80%D0%B0%D0%B2%D0%B4%D0%BE%D0%BF%D0%BE%D0%B4%D0%BE%D0%B1%D0%B8%D1%8F)\n",
    "4. [Лекция про метод максимума правдоподобия](https://www.youtube.com/watch?v=ewe27he5hTY&ab_channel=%D0%91%D0%9E%D0%A0%D0%98%D0%A1%D0%91%D0%9E%D0%AF%D0%A0%D0%A8%D0%98%D0%9D%D0%9E%D0%92%D0%A1%D0%9E%D0%94%D0%9D%D0%90%D0%9D%D0%90%D0%A3%D0%9A%D0%98)\n",
    "5. [Cross Entropy and Log Likelihood](http://www.awebb.info/probability/2017/05/18/cross-entropy-and-log-likelihood.html)\n",
    "6. [Энтропия и кросс-энтропия](https://dementiy.github.io/notes/cross-entropy/)\n",
    "7. [Визуальная теория информации (часть 2)](https://habr.com/ru/articles/484756/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такое функция потерь\n",
    "\n",
    "Функция потерь (loss function) — скалярная функция, которая показывает \"стоимость\" ошибки алгоритма в задаче: $\\mathcal{L}(\\hat y, y)$, где $y$ - фактическое выходное значение, а $\\hat y$ - прогноз.\n",
    "\n",
    "Свойства функции потерь:\n",
    "* Она дифференцируемая\n",
    "    * Если же в каком-то небольшом множестве точек производная функции потерь не определена, мы можем ее доопределить\n",
    "* Не вырожденная, то есть не принимает значение ноль практически во всех своих точках"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Откуда берутся функции потерь\n",
    "\n",
    "**Метод максимального правдоподобия** (MLE — maximum likelihood estimation) — это метод оценивания неизвестного параметра путём максимизации функции правдоподобия. Основан на предположении о том, что вся информация о статистической выборке содержится в функции правдоподобия.\n",
    "\n",
    "Метод максимального правдоподобия помогает получать различные функции потерь из предположений о том, как распределены данные. Будем считать, что объекты $x_i$ ($i=1 \\ldots N$) в некоторой задаче машинного обучения были получены независимо друг от друга, то есть $p(x_1, x_2) = p(x_1)p(x_2)$. Тогда вероятность получения такого набора: $\\prod_{i=1}^N p_\\theta(x_i)$.\n",
    "\n",
    "Выборка $x_i$ была порождена распределением $P_{\\theta}$, где $\\theta \\in \\Theta$  — неизвестные параметры (например, веса нейронной сети). Нам нужно каким-то образом подобрать параметры $\\theta$ так, чтобы получить верные ответы для объекта $x_{N+1}$, про который мы предполгаем, что он был сгенерирован той же функцией распределения. То есть, подобрать некоторое распределение $Q_{\\theta}$.\n",
    "\n",
    "Как подобрать параметры, учитывая, что наши измерения содержат шум (погрешности измерения)? Нужно написать такую функцию, передав в которую параметры мы получим, что вероятность имеющегося набора данных $x_i$ является максимальной, то есть $\\prod_{i=1}^N p_\\theta(x_i) \\rightarrow \\max_\\theta$\n",
    "\n",
    "Эта функция называется **функцией правдоподобия**: $L(\\mathbf {x} \\mid \\theta )\\colon \\Theta \\to \\mathbb {R}$, где $\\mathbf {x} \\in \\mathbb {R}^{n}$. Тогда точечная оценка $\\hat\\theta$ максимального правдоподобия является оценкой этих параметров:\n",
    "\n",
    "$$\\hat\\theta = \\text{argmax}_{\\theta \\in \\Theta} L(x_1 \\ldots x_N \\mid \\theta) = \\text{argmax}_{\\theta \\in \\Theta} \\prod_{i=1}^N p_\\theta(x_i)$$\n",
    "\n",
    "Чтобы упростить задачу, перейдем от максимизации произведения к максимизации суммы при помощи логарифмирования. Это работает благодаря особенностям функции логарифма, которая монотонно возрастает, то есть \"сохраняет\" максимум для последовтельности чисел.\n",
    "\n",
    "$$\\hat\\theta = \\text{argmax}_{\\theta \\in \\Theta} \\sum_{i=1}^N \\log{(p_\\theta(x_i))}$$\n",
    "\n",
    "Например, мы решаем задачу **регрессии**, и $x_i$ были порождены нормальным распределением: $p_\\theta(x_i) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{-(x-\\mu)^2}{2 \\sigma^2}}$. Пусть $\\theta = \\mu$ (забудем пока, что $\\sigma$ - тоже настраиваемый параметр). Тогда:\n",
    "\n",
    "$$\\sum_{i=1}^N log{(p_\\mu(x_i))} = -N log{(\\sqrt{2 \\pi})} - N log{(\\sigma)} - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^N (x_i - \\mu)^2 \\rightarrow \\max_\\mu$$\n",
    "\n",
    "Несложно заметить, что максимизацию этого выражения можно свести к минимизации выражения $\\sum_{i=1}^N (x_i - \\mu)^2$, то есть **методу наименьших квадратов** или **функции ошибки MSE**.\n",
    "\n",
    "Пусть теперь мы решаем задачу **бинарной классификации**, и $x_i$ были порождены биномиальным распределением с настраиваемым параметром $p$, соответствующая функция вероятности того, что класс 1 встретится среди примеров $K$ раз: $P(X = K) = C_N^K p^K (1 - p)^{N-K}$. Тогда:\n",
    "\n",
    "$$\\sum_{i=1}^N log{(C_N^K p^K (1 - p)^{N-K})} = log{(C_N^K)} + K log{(p)} + (N-K)log{(1-p)}$$\n",
    "\n",
    "Отбросим слагаемые, которые не зависят от $p$ и получим, что:\n",
    "\n",
    "$$\\frac{K}{N} log{(p)} + (1 - \\frac{K}{N}) log{(1 - p)} \\rightarrow \\max_p$$\n",
    "\n",
    "Также можно показать, что производная по $p$ от полученного выражения:\n",
    "\n",
    "$$\\frac{K}{N} \\frac{1}{p} - (1 - \\frac{K}{N}) \\frac{1}{1-p} = 0 \\Longrightarrow p = \\frac{K}{N}$$\n",
    "\n",
    "Таким образом, мы получили **функцию ошибки бинарную кросс-энтропию**: \n",
    "\n",
    "$$\\mathcal{L}(p, \\hat p) = \\hat p log{(p)} + (1 - \\hat p) log{(1 - p)}$$\n",
    "\n",
    "где $\\hat p = \\frac{K}{N}$ - таргетное значение (смотри постановку задачи), а $p$ - предполагаемая принадлежность к классу 1.\n",
    "\n",
    "> Замечание!\n",
    "\n",
    "Бинарная кросс-энтропия тесно связана с **дивергенцией Кульбака-Лейбнера** - несимметричной мерой разницы между двумя вероятностными распределениями. В теории информации это количеством информации, которое теряется при приближении истинного распределения $P$ с помощью распределения-приближения $Q$:\n",
    "$$KL(P||Q) = \\int \\log{\\frac{dP}{dQ}dP},$$\n",
    "где интеграл берется по всему пространству исходов, которое у $P$ и $Q$ должно быть общее.\n",
    "\n",
    "Подробнее можно [почитать у Дьяконова](https://alexanderdyakonov.wordpress.com/2018/03/12/%d0%bb%d0%be%d0%b3%d0%b8%d1%81%d1%82%d0%b8%d1%87%d0%b5%d1%81%d0%ba%d0%b0%d1%8f-%d1%84%d1%83%d0%bd%d0%ba%d1%86%d0%b8%d1%8f-%d0%be%d1%88%d0%b8%d0%b1%d0%ba%d0%b8/#more-6139)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции ошибки для задачи регрессии\n",
    "\n",
    "Пример функции активации выходного слоя $\\hat y$ - **линейная**.\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "$\\mathcal{L}(\\hat y, y) = (\\hat y - y)^2$\n",
    "\n",
    "Residual sum of squares: $RSS = \\sum (\\hat y - y)^2$\n",
    "\n",
    "В Pytorch: nn.MSELoss\n",
    "\n",
    "### Mean Abosolute Error (MAE)\n",
    "\n",
    "$\\mathcal{L}(\\hat y, y) = |\\hat y - y|$\n",
    "\n",
    "В Pytorch: nn.L1Loss\n",
    "\n",
    "### Mean Squared Logarithmic Error (MSLE)\n",
    "\n",
    "$\\mathcal{L}(\\hat y, y) = (log{(y + 1)} - log{(\\hat y + 1)})^2$\n",
    "\n",
    "### HuberLoss\n",
    "\n",
    "Комбинация $L_1$ и $L_2$ лосс-функций, более устойчивая к выбросам.\n",
    "\n",
    "$L_{\\delta }(\\hat y, y)={\\begin{cases}{\\frac {1}{2}}(y - \\hat y)^{2}&{\\text{if}}|y-\\hat y|\\leq \\delta ,\\\\\\delta \\,|y-\\hat y|-{\\frac {1}{2}}\\delta ^{2}&{\\text{if}} |y-\\hat y|> \\delta\\end{cases}}$\n",
    "\n",
    "В Pytorch: nn.HuberLoss\n",
    "\n",
    "### SmoothL1Loss\n",
    "\n",
    "Комбинация $L_1$ и $L_2$ лосс-функций, более устойчивая к выбросам.\n",
    "\n",
    "$L_{\\delta }(\\hat y, y)={\\begin{cases}{\\frac {1}{2\\delta}}(y - \\hat y)^{2}&{\\text{if}}|y-\\hat y|\\leq \\delta ,\\\\|y-\\hat y|-{\\frac{\\delta}{2}}&{\\text{if}} |y-\\hat y|> \\delta\\end{cases}}$\n",
    "\n",
    "В Pytorch: nn.SmoothL1Loss\n",
    "\n",
    "<img src=\"pictures/regr_losses.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции ошибки для задачи бинарной классификации\n",
    "\n",
    "Пример функции активации выходного слоя: **сигмоидная** ($\\hat y = \\sigma(z)$). Она хорошо подходит для интерпретации вероятности, так как ее значения находятся в интервале от 0 до 1.\n",
    "\n",
    "Функции активации из предыдущего раздела не подойдут в этой задаче, так как их использование приведет к [параличу нейронной сети](https://stepik.org/lesson/204985/step/5?auth=login&unit=178760).\n",
    "\n",
    "### [0-1 loss](https://www.baeldung.com/cs/ai-0-1-loss-function)\n",
    "\n",
    "Число верных предсказаний (accuracy).\n",
    "\n",
    "$\\mathcal{L}(\\hat y, y) = [sign(\\hat y) = y]$\n",
    "\n",
    "Является кусочно-постоянной функцией, поэтому плохо отражает небольшие изменения в работе модели и плохо пригодна для оптимизации.\n",
    "\n",
    "### Бинарная кросс-энтропия (BCE - Binary Cross-Entropy)\n",
    "\n",
    "$\\mathcal{L}(\\hat y, y) = y log(\\hat y) + (1 - y) log(1 - \\hat y)$\n",
    "\n",
    "В Pytorch: nn.BCELoss\n",
    "\n",
    "### Hinge Loss\n",
    "\n",
    "$\\mathcal{L}(\\hat y, y) = \\max{(0, 1 - \\hat y y)}$\n",
    "\n",
    "Кусочно-линейная (SVM)\n",
    "\n",
    "### Squared Hinge Loss\n",
    "\n",
    "$\\mathcal{L}(\\hat y, y) = \\max{(0, 1 - \\hat y y)^2}$\n",
    "\n",
    "### BCEWithLogitsLoss\n",
    "\n",
    "Комбинация слоя с сигмоидой и BCELoss.\n",
    "\n",
    "В Pytorch: nn.BCEWithLogitsLoss\n",
    "\n",
    "<img src=\"pictures/clf_losses.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции ошибки для задачи многоклассовой классификации\n",
    "\n",
    "Пример функции активации выходного слоя: **софтмакс**.\n",
    "\n",
    "### [Cross-Entropy Loss (logloss)](https://en.wikipedia.org/wiki/Cross_entropy)\n",
    "\n",
    "$\\mathcal{L}(\\hat y, y) = - \\sum_{i=1}^{N classes} y_i log{(\\hat y)}$\n",
    "\n",
    "В Pytorch: nn.CrossEntropyLoss\n",
    "\n",
    "### Дивергенция Кульбака-Лейбнера\n",
    "\n",
    "$\\mathcal{L}(\\hat y, y) = y (log (y) - log (\\hat y))$\n",
    "\n",
    "В Pytorch: nn.KLDivLoss\n",
    "\n",
    "### Sparse Multiclass Cross-Entropy Loss\n",
    "\n",
    "В Pytorch:\n",
    "* nn.NLLLoss\n",
    "* nn.PoissonNLLLoss\n",
    "* nn.GaussianNLLLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что еще есть в pytorch?\n",
    "\n",
    "* nn.HingeEmbeddingLoss\n",
    "* nn.CTCLoss\n",
    "* nn.MarginRankingLoss\n",
    "* nn.MultiLabelMarginLoss\n",
    "* nn.SoftMarginLoss\n",
    "* nn.MultiLabelSoftMarginLoss\n",
    "* nn.CosineEmbeddingLoss\n",
    "* nn.MultiMarginLoss\n",
    "* nn.TripletMarginLoss\n",
    "* nn.TripletMarginWithDistanceLoss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
