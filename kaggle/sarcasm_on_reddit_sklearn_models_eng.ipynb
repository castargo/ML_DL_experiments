{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn models for sarcasm on Reddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import (KFold, GridSearchCV, \n",
    "                                     train_test_split, cross_val_score)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sarcsdet.configs.sklearn_models_config import *\n",
    "from sarcsdet.utils.count_model_metrics import get_best_model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/Sarcasm_on_Reddit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(os.path.join(data_path, 'train-balanced-sarcasm-ling_feat.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Процент датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10108, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = df.sample(frac=0.01)\n",
    "dft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор параметров модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 3), \n",
    "    max_features=50000,\n",
    "    min_df=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(dft.comment_tokenized)\n",
    "y = dft.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {\n",
    "    'AUC': 'roc_auc', \n",
    "    'Accuracy': 'accuracy', \n",
    "    'F1': 'f1', \n",
    "    'F1_micro': 'f1_micro', \n",
    "    'F1_macro': 'f1_macro'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подбор параметров для SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    'C': np.logspace(-5, 5, 5),\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'degree': [3, 5, 9, 12],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(\n",
    "    clf, grid, scoring=scoring, \n",
    "    refit='Accuracy', cv=cv, \n",
    "    verbose=10, n_jobs=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed: 21.1min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed: 44.7min\n",
      "[Parallel(n_jobs=3)]: Done 800 out of 800 | elapsed: 45.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='warn', n_jobs=3,\n",
       "             param_...rray([1.00000000e-05, 3.16227766e-03, 1.00000000e+00, 3.16227766e+02,\n",
       "       1.00000000e+05]),\n",
       "                         'degree': [3, 5, 9, 12], 'gamma': ['scale', 'auto'],\n",
       "                         'kernel': ['linear', 'poly', 'rbf', 'sigmoid']},\n",
       "             pre_dispatch='2*n_jobs', refit='Accuracy',\n",
       "             return_train_score=False,\n",
       "             scoring={'AUC': 'roc_auc', 'Accuracy': 'accuracy', 'F1': 'f1',\n",
       "                      'F1_macro': 'f1_macro', 'F1_micro': 'f1_micro'},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_estimator = gs.best_estimator_\n",
    "best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59423\n",
      "0.63608\n",
      "0.63504\n"
     ]
    }
   ],
   "source": [
    "get_best_model_metrics(X, y, cv, best_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf = MLPClassifier(random_state=8, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_grid = {\n",
    "    'hidden_layer_sizes': [\n",
    "        [64, 64], \n",
    "        [128, 128], [128, 128, 128], \n",
    "        [256, 256], [256, 256, 256]\n",
    "    ],\n",
    "    'activation': ['identity', 'logistic', 'relu'],\n",
    "    'solver': ['adam', 'lbfgs']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_gs = GridSearchCV(\n",
    "    grid_clf, nn_grid, scoring=scoring, \n",
    "    refit='Accuracy', cv=cv, verbose=1, \n",
    "    n_jobs=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed: 42.7min\n",
      "[Parallel(n_jobs=6)]: Done 150 out of 150 | elapsed: 217.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                     batch_size='auto', beta_1=0.9,\n",
       "                                     beta_2=0.999, early_stopping=True,\n",
       "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
       "                                     learning_rate='constant',\n",
       "                                     learning_rate_init=0.001, max_iter=200,\n",
       "                                     momentum=0.9, n_iter_no_change=10,...\n",
       "             iid='warn', n_jobs=6,\n",
       "             param_grid={'activation': ['identity', 'logistic', 'relu'],\n",
       "                         'hidden_layer_sizes': [[64, 64], [128, 128],\n",
       "                                                [128, 128, 128], [256, 256],\n",
       "                                                [256, 256, 256]],\n",
       "                         'solver': ['adam', 'lbfgs']},\n",
       "             pre_dispatch='2*n_jobs', refit='Accuracy',\n",
       "             return_train_score=False,\n",
       "             scoring={'AUC': 'roc_auc', 'Accuracy': 'accuracy', 'F1': 'f1',\n",
       "                      'F1_macro': 'f1_macro', 'F1_micro': 'f1_micro'},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "              beta_1=0.9, beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "              hidden_layer_sizes=[256, 256, 256], learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=8, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_estimator = nn_gs.best_estimator_\n",
    "best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.58986\n",
      "0.62928\n",
      "0.62772\n"
     ]
    }
   ],
   "source": [
    "get_best_model_metrics(X, y, cv, best_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение моделей из Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(**svm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_auto_sklearn = MLPClassifier(**nn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(**logit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sarcsdet.embeddings.mean_ft_v import MeanFastTextEmbeddingVectorizer\n",
    "from sarcsdet.embeddings.mean_glove_v import MeanGloVeEmbeddingVectorizer\n",
    "from sarcsdet.embeddings.mean_w2v_v import MeanW2VEmbeddingVectorizer\n",
    "from sarcsdet.embeddings.tfidf_v import TfidfEmbeddingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [\n",
    "    ('tfidf', tfidf), \n",
    "    ('word2vec vectorizer', MeanW2VEmbeddingVectorizer()), \n",
    "    ('glove vectorizer', MeanGloVeEmbeddingVectorizer()),\n",
    "    ('tfidf embedding vectorizer', TfidfEmbeddingVectorizer()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessors = {}\n",
    "\n",
    "for model in embeddings:\n",
    "    preprocessors[(model[0], 'comment')] = ColumnTransformer([\n",
    "        ('c', model[1], 'comment_tokenized'),\n",
    "    ])\n",
    "    \n",
    "    preprocessors[(model[0], 'comment + funny_mark')] = ColumnTransformer([\n",
    "            ('c', model[1], 'comment_tokenized'),\n",
    "            ('funny_mark', StandardScaler(), ['funny_mark', ]),\n",
    "    ])\n",
    "    \n",
    "    preprocessors[(model[0], 'comment + interjections')] = ColumnTransformer([\n",
    "            ('c', model[1], 'comment_tokenized'),\n",
    "            ('interjections', StandardScaler(), ['interjections', ]),\n",
    "    ])\n",
    "    \n",
    "    preprocessors[(model[0], 'comment + c_punctuation')] = ColumnTransformer([\n",
    "            ('c', model[1], 'comment_tokenized'),\n",
    "            (\n",
    "                'c_punct', \n",
    "                StandardScaler(), \n",
    "                [\n",
    "                    'c_exclamation_mark', 'c_question_mark', 'c_quotes', \n",
    "                    'c_three_dots', 'c_openeing_bracket', 'c_closing_bracket',\n",
    "                ]\n",
    "            ),\n",
    "    ])\n",
    "    \n",
    "    preprocessors[(model[0], 'comment + all_c_feats')] = ColumnTransformer([\n",
    "            ('c', model[1], 'comment_tokenized'),\n",
    "            (\n",
    "                'all_c_feats', \n",
    "                StandardScaler(), \n",
    "                [\n",
    "                    'funny_mark', 'interjections', 'c_exclamation_mark', \n",
    "                    'c_question_mark', 'c_quotes', 'c_three_dots', \n",
    "                    'c_openeing_bracket', 'c_closing_bracket',\n",
    "                ]\n",
    "            ),\n",
    "    ])\n",
    "    \n",
    "    preprocessors[(model[0], 'comment + par_comment')] = ColumnTransformer([\n",
    "            ('c', model[1], 'comment_tokenized'),\n",
    "            ('pc', TfidfVectorizer(), 'parent_comment_tokenized'),\n",
    "    ])\n",
    "    \n",
    "    preprocessors[(model[0], 'comment + par_comment + c_pc_punctuation')] = ColumnTransformer([\n",
    "            ('c', model[1], 'comment_tokenized'),\n",
    "            ('pc', model[1], 'parent_comment_tokenized'),\n",
    "            (\n",
    "                'category', \n",
    "                StandardScaler(), \n",
    "                [\n",
    "                    'c_exclamation_mark', 'c_question_mark', 'c_quotes', \n",
    "                    'c_three_dots', 'c_openeing_bracket', 'c_closing_bracket',\n",
    "                    'pc_exclamation_mark', 'pc_question_mark', 'pc_quotes', \n",
    "                    'pc_three_dots', 'pc_openeing_bracket', 'pc_closing_bracket'\n",
    "                ]\n",
    "            ),\n",
    "    ])\n",
    "    \n",
    "    preprocessors[(model[0], 'comment + par_comment + all_feats')] = ColumnTransformer([\n",
    "            ('c', model[1], 'comment_tokenized'),\n",
    "            ('pc', model[1], 'parent_comment_tokenized'),\n",
    "            (\n",
    "                'category', \n",
    "                StandardScaler(), \n",
    "                [\n",
    "                    'funny_mark', 'interjections', 'c_exclamation_mark', \n",
    "                    'c_question_mark', 'c_quotes', 'c_three_dots', \n",
    "                    'c_openeing_bracket', 'c_closing_bracket',\n",
    "                    'pc_exclamation_mark', 'pc_question_mark', 'pc_quotes', \n",
    "                    'pc_three_dots', 'pc_openeing_bracket', 'pc_closing_bracket'\n",
    "                ]\n",
    "            ),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {}\n",
    "\n",
    "for prep in preprocessors.keys():\n",
    "    pipelines[('logit', prep[0], prep[1])] = Pipeline([\n",
    "        (\"preprocessor\", preprocessors[prep]),\n",
    "        (\"LogReg\", logit),\n",
    "    ])\n",
    "    \n",
    "    pipelines[('svm', prep[0], prep[1])] = Pipeline([\n",
    "        (\"preprocessor\", preprocessors[prep]),\n",
    "        (\"SVM\", svm),\n",
    "    ])\n",
    "    \n",
    "    pipelines[('sklearn_nn', prep[0], prep[1])] = Pipeline([\n",
    "        (\"preprocessor\", preprocessors[prep]),\n",
    "        (\"NN\", nn_auto_sklearn),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_tfidf_comment done: mean acc: 0.627; spent time: 1.25\n",
      "svm_tfidf_comment done: mean acc: 0.628; spent time: 19.6\n",
      "sklearn_nn_tfidf_comment done: mean acc: 0.623; spent time: 2.8e+02\n",
      "logit_tfidf_comment + funny_mark done: mean acc: 0.627; spent time: 1.55\n",
      "svm_tfidf_comment + funny_mark done: mean acc: 0.628; spent time: 21.8\n",
      "sklearn_nn_tfidf_comment + funny_mark done: mean acc: 0.631; spent time: 2.89e+02\n",
      "logit_tfidf_comment + interjections done: mean acc: 0.627; spent time: 1.55\n",
      "svm_tfidf_comment + interjections done: mean acc: 0.626; spent time: 21.8\n",
      "sklearn_nn_tfidf_comment + interjections done: mean acc: 0.627; spent time: 2.87e+02\n",
      "logit_tfidf_comment + c_punctuation done: mean acc: 0.642; spent time: 1.77\n",
      "svm_tfidf_comment + c_punctuation done: mean acc: 0.619; spent time: 23.7\n",
      "sklearn_nn_tfidf_comment + c_punctuation done: mean acc: 0.634; spent time: 2.81e+02\n",
      "logit_tfidf_comment + all_c_feats done: mean acc: 0.643; spent time: 1.76\n",
      "svm_tfidf_comment + all_c_feats done: mean acc: 0.615; spent time: 24.8\n",
      "sklearn_nn_tfidf_comment + all_c_feats done: mean acc: 0.634; spent time: 2.73e+02\n",
      "logit_tfidf_comment + par_comment done: mean acc: 0.623; spent time: 2.75\n",
      "svm_tfidf_comment + par_comment done: mean acc: 0.631; spent time: 51.1\n",
      "sklearn_nn_tfidf_comment + par_comment done: mean acc: 0.615; spent time: 1.01e+03\n",
      "logit_tfidf_comment + par_comment + c_pc_punctuation done: mean acc: 0.636; spent time: 6.46\n",
      "svm_tfidf_comment + par_comment + c_pc_punctuation done: mean acc: 0.587; spent time: 56.7\n",
      "sklearn_nn_tfidf_comment + par_comment + c_pc_punctuation done: mean acc: 0.635; spent time: 6.91e+02\n",
      "logit_tfidf_comment + par_comment + all_feats done: mean acc: 0.638; spent time: 6.37\n",
      "svm_tfidf_comment + par_comment + all_feats done: mean acc: 0.582; spent time: 57.6\n",
      "sklearn_nn_tfidf_comment + par_comment + all_feats done: mean acc: 0.626; spent time: 7.07e+02\n",
      "logit_word2vec vectorizer_comment done: mean acc: 0.521; spent time: 2.96\n",
      "svm_word2vec vectorizer_comment done: mean acc: 0.524; spent time: 44.9\n",
      "sklearn_nn_word2vec vectorizer_comment done: mean acc: 0.509; spent time: 20.0\n",
      "logit_word2vec vectorizer_comment + funny_mark done: mean acc: 0.524; spent time: 2.95\n",
      "svm_word2vec vectorizer_comment + funny_mark done: mean acc: 0.518; spent time: 46.0\n",
      "sklearn_nn_word2vec vectorizer_comment + funny_mark done: mean acc: 0.511; spent time: 18.6\n",
      "logit_word2vec vectorizer_comment + interjections done: mean acc: 0.525; spent time: 2.95\n",
      "svm_word2vec vectorizer_comment + interjections done: mean acc: 0.517; spent time: 46.1\n",
      "sklearn_nn_word2vec vectorizer_comment + interjections done: mean acc: 0.511; spent time: 20.1\n",
      "logit_word2vec vectorizer_comment + c_punctuation done: mean acc: 0.533; spent time: 2.98\n",
      "svm_word2vec vectorizer_comment + c_punctuation done: mean acc: 0.548; spent time: 45.4\n",
      "sklearn_nn_word2vec vectorizer_comment + c_punctuation done: mean acc: 0.535; spent time: 30.5\n",
      "logit_word2vec vectorizer_comment + all_c_feats done: mean acc: 0.537; spent time: 2.98\n",
      "svm_word2vec vectorizer_comment + all_c_feats done: mean acc: 0.553; spent time: 45.7\n",
      "sklearn_nn_word2vec vectorizer_comment + all_c_feats done: mean acc: 0.542; spent time: 29.4\n",
      "logit_word2vec vectorizer_comment + par_comment done: mean acc: 0.535; spent time: 5.28\n",
      "svm_word2vec vectorizer_comment + par_comment done: mean acc: 0.542; spent time: 98.5\n",
      "sklearn_nn_word2vec vectorizer_comment + par_comment done: mean acc: 0.529; spent time: 1.11e+03\n",
      "logit_word2vec vectorizer_comment + par_comment + c_pc_punctuation done: mean acc: 0.532; spent time: 8.55\n",
      "svm_word2vec vectorizer_comment + par_comment + c_pc_punctuation done: mean acc: 0.555; spent time: 95.1\n",
      "sklearn_nn_word2vec vectorizer_comment + par_comment + c_pc_punctuation done: mean acc: 0.533; spent time: 40.9\n",
      "logit_word2vec vectorizer_comment + par_comment + all_feats done: mean acc: 0.536; spent time: 8.55\n",
      "svm_word2vec vectorizer_comment + par_comment + all_feats done: mean acc: 0.558; spent time: 94.5\n",
      "sklearn_nn_word2vec vectorizer_comment + par_comment + all_feats done: mean acc: 0.543; spent time: 35.3\n",
      "logit_glove vectorizer_comment done: mean acc: 0.509; spent time: 0.936\n",
      "svm_glove vectorizer_comment done: mean acc: 0.509; spent time: 1.17e+02\n",
      "sklearn_nn_glove vectorizer_comment done: mean acc: 0.509; spent time: 20.2\n",
      "logit_glove vectorizer_comment + funny_mark done: mean acc: 0.511; spent time: 0.945\n",
      "svm_glove vectorizer_comment + funny_mark done: mean acc: 0.511; spent time: 1.17e+02\n",
      "sklearn_nn_glove vectorizer_comment + funny_mark done: mean acc: 0.509; spent time: 22.4\n",
      "logit_glove vectorizer_comment + interjections done: mean acc: 0.509; spent time: 0.961\n",
      "svm_glove vectorizer_comment + interjections done: mean acc: 0.509; spent time: 1.17e+02\n",
      "sklearn_nn_glove vectorizer_comment + interjections done: mean acc: 0.509; spent time: 18.8\n",
      "logit_glove vectorizer_comment + c_punctuation done: mean acc: 0.537; spent time: 0.98\n",
      "svm_glove vectorizer_comment + c_punctuation done: mean acc: 0.548; spent time: 1.17e+02\n",
      "sklearn_nn_glove vectorizer_comment + c_punctuation done: mean acc: 0.536; spent time: 30.7\n",
      "logit_glove vectorizer_comment + all_c_feats done: mean acc: 0.541; spent time: 1.01\n",
      "svm_glove vectorizer_comment + all_c_feats done: mean acc: 0.553; spent time: 1.18e+02\n",
      "sklearn_nn_glove vectorizer_comment + all_c_feats done: mean acc: 0.542; spent time: 30.1\n",
      "logit_glove vectorizer_comment + par_comment done: mean acc: 0.532; spent time: 2.34\n",
      "svm_glove vectorizer_comment + par_comment done: mean acc: 0.54; spent time: 35.8\n",
      "sklearn_nn_glove vectorizer_comment + par_comment done: mean acc: 0.537; spent time: 9.04e+02\n",
      "logit_glove vectorizer_comment + par_comment + c_pc_punctuation done: mean acc: 0.536; spent time: 2.05\n",
      "svm_glove vectorizer_comment + par_comment + c_pc_punctuation done: mean acc: 0.554; spent time: 2.35e+02\n",
      "sklearn_nn_glove vectorizer_comment + par_comment + c_pc_punctuation done: mean acc: 0.536; spent time: 67.3\n",
      "logit_glove vectorizer_comment + par_comment + all_feats done: mean acc: 0.539; spent time: 2.09\n",
      "svm_glove vectorizer_comment + par_comment + all_feats done: mean acc: 0.557; spent time: 2.34e+02\n",
      "sklearn_nn_glove vectorizer_comment + par_comment + all_feats done: mean acc: 0.54; spent time: 48.1\n",
      "logit_tfidf embedding vectorizer_comment done: mean acc: 0.522; spent time: 13.2\n",
      "svm_tfidf embedding vectorizer_comment done: mean acc: 0.524; spent time: 57.3\n",
      "sklearn_nn_tfidf embedding vectorizer_comment done: mean acc: 0.508; spent time: 58.4\n",
      "logit_tfidf embedding vectorizer_comment + funny_mark done: mean acc: 0.523; spent time: 13.2\n",
      "svm_tfidf embedding vectorizer_comment + funny_mark done: mean acc: 0.522; spent time: 55.5\n",
      "sklearn_nn_tfidf embedding vectorizer_comment + funny_mark done: mean acc: 0.51; spent time: 46.0\n",
      "logit_tfidf embedding vectorizer_comment + interjections done: mean acc: 0.521; spent time: 13.2\n",
      "svm_tfidf embedding vectorizer_comment + interjections done: mean acc: 0.522; spent time: 55.7\n",
      "sklearn_nn_tfidf embedding vectorizer_comment + interjections done: mean acc: 0.506; spent time: 42.2\n",
      "logit_tfidf embedding vectorizer_comment + c_punctuation done: mean acc: 0.541; spent time: 13.4\n",
      "svm_tfidf embedding vectorizer_comment + c_punctuation done: mean acc: 0.55; spent time: 57.2\n",
      "sklearn_nn_tfidf embedding vectorizer_comment + c_punctuation done: mean acc: 0.527; spent time: 40.2\n",
      "logit_tfidf embedding vectorizer_comment + all_c_feats done: mean acc: 0.543; spent time: 13.3\n",
      "svm_tfidf embedding vectorizer_comment + all_c_feats done: mean acc: 0.553; spent time: 56.5\n",
      "sklearn_nn_tfidf embedding vectorizer_comment + all_c_feats done: mean acc: 0.539; spent time: 37.2\n",
      "logit_tfidf embedding vectorizer_comment + par_comment done: mean acc: 0.538; spent time: 16.4\n",
      "svm_tfidf embedding vectorizer_comment + par_comment done: mean acc: 0.532; spent time: 1.05e+02\n",
      "sklearn_nn_tfidf embedding vectorizer_comment + par_comment done: mean acc: 0.526; spent time: 1.05e+03\n",
      "logit_tfidf embedding vectorizer_comment + par_comment + c_pc_punctuation done: mean acc: 0.536; spent time: 40.1\n",
      "svm_tfidf embedding vectorizer_comment + par_comment + c_pc_punctuation done: mean acc: 0.544; spent time: 1.22e+02\n",
      "sklearn_nn_tfidf embedding vectorizer_comment + par_comment + c_pc_punctuation done: mean acc: 0.536; spent time: 75.5\n",
      "logit_tfidf embedding vectorizer_comment + par_comment + all_feats done: mean acc: 0.54; spent time: 39.0\n",
      "svm_tfidf embedding vectorizer_comment + par_comment + all_feats done: mean acc: 0.542; spent time: 1.23e+02\n",
      "sklearn_nn_tfidf embedding vectorizer_comment + par_comment + all_feats done: mean acc: 0.539; spent time: 71.4\n"
     ]
    }
   ],
   "source": [
    "unsorted_scores = []\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    start_time = time.time()\n",
    "    print(\"{:15} \".format(\"_\".join(name)), end=\"\")\n",
    "    cvs = cross_val_score(pipe, dft, dft.label, cv=5, n_jobs=1, verbose=0, scoring=\"accuracy\").mean()\n",
    "    print(\"done: mean acc: {:.3}; spent time: {:.3}\".format(cvs, time.time() - start_time))\n",
    "    unsorted_scores.append((name, cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model       embedding                   data                                        score\n",
      "----------  --------------------------  ----------------------------------------  -------\n",
      "logit       tfidf                       comment + all_c_feats                      0.6430\n",
      "logit       tfidf                       comment + c_punctuation                    0.6418\n",
      "logit       tfidf                       comment + par_comment + all_feats          0.6376\n",
      "logit       tfidf                       comment + par_comment + c_pc_punctuation   0.6359\n",
      "sklearn_nn  tfidf                       comment + par_comment + c_pc_punctuation   0.6353\n",
      "sklearn_nn  tfidf                       comment + all_c_feats                      0.6339\n",
      "sklearn_nn  tfidf                       comment + c_punctuation                    0.6336\n",
      "sklearn_nn  tfidf                       comment + funny_mark                       0.6312\n",
      "svm         tfidf                       comment + par_comment                      0.6311\n",
      "svm         tfidf                       comment                                    0.6278\n",
      "svm         tfidf                       comment + funny_mark                       0.6275\n",
      "logit       tfidf                       comment + funny_mark                       0.6274\n",
      "logit       tfidf                       comment                                    0.6272\n",
      "sklearn_nn  tfidf                       comment + interjections                    0.6272\n",
      "logit       tfidf                       comment + interjections                    0.6266\n",
      "svm         tfidf                       comment + interjections                    0.6259\n",
      "sklearn_nn  tfidf                       comment + par_comment + all_feats          0.6258\n",
      "sklearn_nn  tfidf                       comment                                    0.6232\n",
      "logit       tfidf                       comment + par_comment                      0.6229\n",
      "svm         tfidf                       comment + c_punctuation                    0.6187\n",
      "svm         tfidf                       comment + all_c_feats                      0.6152\n",
      "sklearn_nn  tfidf                       comment + par_comment                      0.6148\n",
      "svm         tfidf                       comment + par_comment + c_pc_punctuation   0.5869\n",
      "svm         tfidf                       comment + par_comment + all_feats          0.5815\n",
      "svm         word2vec vectorizer         comment + par_comment + all_feats          0.5579\n",
      "svm         glove vectorizer            comment + par_comment + all_feats          0.5575\n",
      "svm         word2vec vectorizer         comment + par_comment + c_pc_punctuation   0.5547\n",
      "svm         glove vectorizer            comment + par_comment + c_pc_punctuation   0.5541\n",
      "svm         word2vec vectorizer         comment + all_c_feats                      0.5529\n",
      "svm         glove vectorizer            comment + all_c_feats                      0.5528\n",
      "svm         tfidf embedding vectorizer  comment + all_c_feats                      0.5525\n",
      "svm         tfidf embedding vectorizer  comment + c_punctuation                    0.5504\n",
      "svm         word2vec vectorizer         comment + c_punctuation                    0.5485\n",
      "svm         glove vectorizer            comment + c_punctuation                    0.5477\n",
      "svm         tfidf embedding vectorizer  comment + par_comment + c_pc_punctuation   0.5443\n",
      "sklearn_nn  word2vec vectorizer         comment + par_comment + all_feats          0.5434\n",
      "logit       tfidf embedding vectorizer  comment + all_c_feats                      0.5427\n",
      "svm         tfidf embedding vectorizer  comment + par_comment + all_feats          0.5423\n",
      "sklearn_nn  word2vec vectorizer         comment + all_c_feats                      0.5419\n",
      "sklearn_nn  glove vectorizer            comment + all_c_feats                      0.5417\n",
      "svm         word2vec vectorizer         comment + par_comment                      0.5416\n",
      "logit       glove vectorizer            comment + all_c_feats                      0.5410\n",
      "logit       tfidf embedding vectorizer  comment + c_punctuation                    0.5406\n",
      "logit       tfidf embedding vectorizer  comment + par_comment + all_feats          0.5405\n",
      "sklearn_nn  glove vectorizer            comment + par_comment + all_feats          0.5400\n",
      "svm         glove vectorizer            comment + par_comment                      0.5396\n",
      "logit       glove vectorizer            comment + par_comment + all_feats          0.5390\n",
      "sklearn_nn  tfidf embedding vectorizer  comment + all_c_feats                      0.5390\n",
      "sklearn_nn  tfidf embedding vectorizer  comment + par_comment + all_feats          0.5387\n",
      "logit       tfidf embedding vectorizer  comment + par_comment                      0.5380\n",
      "logit       glove vectorizer            comment + c_punctuation                    0.5371\n",
      "sklearn_nn  glove vectorizer            comment + par_comment                      0.5370\n",
      "logit       word2vec vectorizer         comment + all_c_feats                      0.5366\n",
      "logit       word2vec vectorizer         comment + par_comment + all_feats          0.5361\n",
      "sklearn_nn  glove vectorizer            comment + c_punctuation                    0.5361\n",
      "sklearn_nn  tfidf embedding vectorizer  comment + par_comment + c_pc_punctuation   0.5361\n",
      "logit       glove vectorizer            comment + par_comment + c_pc_punctuation   0.5360\n",
      "logit       tfidf embedding vectorizer  comment + par_comment + c_pc_punctuation   0.5358\n",
      "sklearn_nn  glove vectorizer            comment + par_comment + c_pc_punctuation   0.5355\n",
      "logit       word2vec vectorizer         comment + par_comment                      0.5354\n",
      "sklearn_nn  word2vec vectorizer         comment + c_punctuation                    0.5351\n",
      "sklearn_nn  word2vec vectorizer         comment + par_comment + c_pc_punctuation   0.5332\n",
      "logit       word2vec vectorizer         comment + c_punctuation                    0.5325\n",
      "logit       word2vec vectorizer         comment + par_comment + c_pc_punctuation   0.5323\n",
      "svm         tfidf embedding vectorizer  comment + par_comment                      0.5321\n",
      "logit       glove vectorizer            comment + par_comment                      0.5319\n",
      "sklearn_nn  word2vec vectorizer         comment + par_comment                      0.5287\n",
      "sklearn_nn  tfidf embedding vectorizer  comment + c_punctuation                    0.5269\n",
      "sklearn_nn  tfidf embedding vectorizer  comment + par_comment                      0.5262\n",
      "logit       word2vec vectorizer         comment + interjections                    0.5245\n",
      "logit       word2vec vectorizer         comment + funny_mark                       0.5243\n",
      "svm         word2vec vectorizer         comment                                    0.5239\n",
      "svm         tfidf embedding vectorizer  comment                                    0.5235\n",
      "logit       tfidf embedding vectorizer  comment + funny_mark                       0.5231\n",
      "svm         tfidf embedding vectorizer  comment + funny_mark                       0.5224\n",
      "svm         tfidf embedding vectorizer  comment + interjections                    0.5220\n",
      "logit       tfidf embedding vectorizer  comment                                    0.5218\n",
      "logit       word2vec vectorizer         comment                                    0.5215\n",
      "logit       tfidf embedding vectorizer  comment + interjections                    0.5212\n",
      "svm         word2vec vectorizer         comment + funny_mark                       0.5183\n",
      "svm         word2vec vectorizer         comment + interjections                    0.5173\n",
      "svm         glove vectorizer            comment + funny_mark                       0.5109\n",
      "sklearn_nn  word2vec vectorizer         comment + interjections                    0.5109\n",
      "logit       glove vectorizer            comment + funny_mark                       0.5107\n",
      "sklearn_nn  word2vec vectorizer         comment + funny_mark                       0.5106\n",
      "sklearn_nn  tfidf embedding vectorizer  comment + funny_mark                       0.5100\n",
      "sklearn_nn  word2vec vectorizer         comment                                    0.5093\n",
      "logit       glove vectorizer            comment                                    0.5093\n",
      "svm         glove vectorizer            comment                                    0.5093\n",
      "sklearn_nn  glove vectorizer            comment                                    0.5093\n",
      "logit       glove vectorizer            comment + interjections                    0.5093\n",
      "svm         glove vectorizer            comment + interjections                    0.5093\n",
      "sklearn_nn  glove vectorizer            comment + interjections                    0.5093\n",
      "sklearn_nn  glove vectorizer            comment + funny_mark                       0.5090\n",
      "sklearn_nn  tfidf embedding vectorizer  comment                                    0.5082\n",
      "sklearn_nn  tfidf embedding vectorizer  comment + interjections                    0.5061\n"
     ]
    }
   ],
   "source": [
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])\n",
    "\n",
    "print(tabulate(\n",
    "    [[item[0][0], item[0][1], item[0][2], item[1]] for item in scores], \n",
    "    floatfmt=\".4f\", \n",
    "    headers=(\"model\", \"embedding\", \"data\", \"score\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dft, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение лучшей модели из Sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/Sarcasm_on_Reddit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(os.path.join(data_path, 'train-balanced-sarcasm-ling_feat.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, valid_texts, y_train, y_valid = train_test_split(\n",
    "    df[[\n",
    "        'comment_tokenized', 'funny_mark', 'interjections', \n",
    "        'c_exclamation_mark', 'c_question_mark', 'c_quotes', \n",
    "        'c_three_dots', 'c_openeing_bracket', 'c_closing_bracket'\n",
    "    ]], \n",
    "    df['label'], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 3), \n",
    "    max_features=50000,\n",
    "    min_df=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepr = ColumnTransformer([\n",
    "        ('c', tfidf, 'comment_tokenized'),\n",
    "        (\n",
    "            'all_c_feats', \n",
    "            StandardScaler(), \n",
    "            [\n",
    "                'funny_mark', 'interjections', 'c_exclamation_mark', \n",
    "                'c_question_mark', 'c_quotes', 'c_three_dots', \n",
    "                'c_openeing_bracket', 'c_closing_bracket',\n",
    "            ]\n",
    "        ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(\n",
    "    C=1, solver='lbfgs',\n",
    "    dual=False, max_iter=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_logit_pipeline = Pipeline([(\"preprocessor\", prepr), ('logit', logit)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 38s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('c',\n",
       "                                                  TfidfVectorizer(analyzer='word',\n",
       "                                                                  binary=False,\n",
       "                                                                  decode_error='strict',\n",
       "                                                                  dtype=<class 'numpy.float64'>,\n",
       "                                                                  encoding='utf-8',\n",
       "                                                                  input='content',\n",
       "                                                                  lowercase=True,\n",
       "                                                                  max_df=1.0,\n",
       "                                                                  max_features=50000,\n",
       "                                                                  min_df=2,\n",
       "                                                                  ngram_range...\n",
       "                                                   'c_question_mark',\n",
       "                                                   'c_quotes', 'c_three_dots',\n",
       "                                                   'c_openeing_bracket',\n",
       "                                                   'c_closing_bracket'])],\n",
       "                                   verbose=False)),\n",
       "                ('logit',\n",
       "                 LogisticRegression(C=1, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=2000,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tfidf_logit_pipeline.fit(train_texts, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_pred = tfidf_logit_pipeline.predict(valid_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7003169678718832"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, valid_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
