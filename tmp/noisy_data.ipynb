{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка моделей на шумных данных\n",
    "\n",
    "### Noisy data: classification task\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"300\" height=\"300\" src=\"shakeup2.jpg\"/>\n",
    "</p>\n",
    "\n",
    "### Noisy data: regression task\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"400\" height=\"220\" src=\"shakeup.jpg\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пройтись по ошибкам валидации: интерпретация и визуализация ML-моделей\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"400\" height=\"220\" src=\"output_10_0.png\"/>\n",
    "</p>\n",
    "\n",
    "### Новые инструменты для интерпретации и визуализации ML-моделей\n",
    "\n",
    "#### Yellowbrick — расширение scikit-learn\n",
    "* [Документация](https://www.scikit-yb.org/en/latest/quickstart.html#using-yellowbrick)\n",
    "* [Machine Learning Visualizations with Yellowbrick](https://medium.com/data-science-community-srm/machine-learning-visualizations-with-yellowbrick-3c533955b1b3)\n",
    "\n",
    "#### ELI5\n",
    "Еще одна визуальная библиотека для устранения ошибок в моделях и объяснения прогнозов (совместима со scikit-learn, XGBoost и Keras).\n",
    "* [Документация](https://eli5.readthedocs.io/en/latest/tutorials/sklearn-text.html)\n",
    "\n",
    "#### MLxtend - визуализация данных, сравнение решений и входящих в состав классификаторов\n",
    "* [Документация](http://rasbt.github.io/mlxtend/)\n",
    "\n",
    "#### LIME – пакет для интерпретации прогнозов\n",
    "* [Документация](https://github.com/marcotcr/lime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Важность фичей и их распределение на тренировочной и тестовой выборке\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"600\" height=\"600\" src=\"aleron.jpg\"/>\n",
    "</p>\n",
    "\n",
    "### Population Stability Index\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"600\" height=\"600\" src=\"psi.png\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Переразметить часть данных\n",
    "\n",
    "### ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Статистические критерии: точечные и интервальные оценки\n",
    "\n",
    "Для точечных оценок: непараметрические критерии\n",
    "\n",
    "Для интервальных оценок: бутстрап\n",
    "\n",
    "Литература:\n",
    "* [Statistical Significance Tests for Comparing Machine Learning Algorithms](https://machinelearningmastery.com/statistical-significance-tests-for-comparing-machine-learning-algorithms/)\n",
    "* [Model evaluation, model selection, and algorithm selection in machine learning](https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html)\n",
    "* [Habr: Kaggle Mercedes и кросс-валидация](https://habr.com/ru/company/ods/blog/336168/)\n",
    "* [YouTube: Kaggle Mercedes Benz: предсказание времени тестирования автомобилей](https://www.youtube.com/watch?v=HT3QpRp2ewA&ab_channel=ODSAIRu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling\n",
    "\n",
    "### ???\n",
    "\n",
    "Литература:\n",
    "* [SMOTE for Imbalanced Classification with Python](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)\n",
    "* [Radial-Based oversampling for noisy imbalanced data classification](https://www.sciencedirect.com/science/article/abs/pii/S0925231219301596)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley Additive Explanations\n",
    "\n",
    "Рассматривается вклад признаков в итоговое предсказание модели как задача теории игр: игра – процесс, в котором участвуют две и более сторон, ведущих борьбу за свои интересы. Каждая сторона имеет свою цель и использует собственную стратегию, которая может привести к выигрышу или проигрышу, в зависимости от поведения других игроков. \n",
    "\n",
    "Определить оптимальное распределение выигрыша между игроками можно с помощью вектора Шепли. Он представляет собой распределение, в котором выигрыш каждого игрока равен его среднему вкладу в общее благосостояние при определенном механизме формирования коалиции.\n",
    "\n",
    "Применяя вышеизложенные положения теории игр к интерпретации ML-моделей, можно сделать следующие выводы:\n",
    "* результат обучения с учителем (на основе заданного примера) – это игра;\n",
    "* выигрыш – это разница между матожиданием результата на всех имеющихся примерах и результатом, полученном на заданном примере;\n",
    "* вклады игроков в игру – влияние каждого значения признака на выигрыш, т.е. результат.\n",
    "\n",
    "При расчёте вектора Шепли необходимо формировать коалиции из ограниченного набора признаков. Однако, не каждая ML-модель позволяет просто убрать признак без повторного обучения модели «с нуля». Потому для формирования коалиций обычно не убирают «лишние» признаки, а заменяют их на случайные значения из «фонового» набора данных. Усреднённый результат модели со случайными значениями признака эквивалентен результату модели, в которой этот признак вообще отсутствует.\n",
    "\n",
    "Литература:\n",
    "* [SHAP (SHapley Additive exPlanations)](https://christophm.github.io/interpretable-ml-book/shap.html)\n",
    "* [Shapley Additive Explanations (SHAP) for Average Attributions](https://ema.drwhy.ai/shapley.html)\n",
    "* [Интерпретируй это: метод SHAP в Data Science](https://chernobrovov.ru/articles/interpretiruj-eto-metod-shap-v-data-science.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модели со случайными предсказаниями\n",
    "\n",
    "#### Регрессия\n",
    "Модели со случайными предсказаниями предсказывают распределение признаков. Они устойчивы к выбросам (за счет того, что на сложных примерах меньше штрафуют модель), могут работать с многомодальными данными (можно предсказывать смеси распределений), хорошо работают с нестандартными данными. Используют кросс-энтропийный китерий.\n",
    "\n",
    "#### Классификация\n",
    "Задача Metric Learning: мы хотим построить отображение входных данных в скрытое пространство таким образом, что полученные эмбединги элементов одного класса были близки друг к другу (например, по l2 ли cos), а эмбединги элементов разных классов были далеки друг от друга. Для этого есть специальные функции потерь, например, Contrastive Loss (by Yann Le Cunn), Triplet loss.\n",
    "* Плохо обучается, когда много выбросов. Решение: перейти к предсказанию распределения векторов.\n",
    "\n",
    "Литература:\n",
    "* [Модели со случайными предсказаниями — Иван Карпухин, Тинькофф](https://www.youtube.com/watch?v=-IbiKlAJqSM&ab_channel=IT%27sTinkoff)\n",
    "* [Metric Learning Tips & Tricks](https://towardsdatascience.com/metric-learning-tips-n-tricks-2e4cfee6b75b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
