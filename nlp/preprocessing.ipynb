{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предобработка языка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уровни языка и текста\n",
    "\n",
    "* [NLPub](https://nlpub.ru/)\n",
    "    * [Библиотеки Python для различных задач предобработки текстов](https://nlpub.ru/%D0%9E%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0)\n",
    "* [Автоматическая обработка текстов на естественном языке и анализ данных - Большакова, Воронцов, Ефремова, Клышинский, Лукашевич, Сапин](https://www.hse.ru/data/2017/08/12/1174382135/NLP_and_DA.pdf)\n",
    "\n",
    "**Структурная модель естественного языка учитывает особенности уровней текста:**\n",
    "* Фонологический: звуки\n",
    "* Графематический: символы текста\n",
    "* Морфологический: слова\n",
    "* Лексический: множество лексем\n",
    "* Синтаксический: слова, предложения\n",
    "* Семантический: семы - слова, предложения, текст (слова \"хороший\" и \"нехороший\" отличаются семой отрицания)\n",
    "* Дискурсивный: схематические структуры текстов (текст в целом)\n",
    "\n",
    "Единицы языка:\n",
    "* фонемы (звуки)\n",
    "* морфемы (части слов)\n",
    "* лексемы (слова)\n",
    "\n",
    "Единицы текста (речи):\n",
    "* слова (словоформы)\n",
    "* словосочетания\n",
    "* предложения (фразы)\n",
    "\n",
    "Явления, составляющие сложность в обработке естественного языка:\n",
    "* Синонимия (избыточность)\n",
    "* Полисемия и омонимия (многозначность)\n",
    "\n",
    "**Лингвистический процессор** — программа обработки текстов на ЕЯ в рамках конкретной прикладной задачи. Его основой является модель языка и/или текста (модели зависят от ЕЯ и прикладной задачи). ЛП опирается на лингвистические ресурсы. При этом этапы обработки ∼ уровни языка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этапы обработки текста\n",
    "1. **Графематический анализ и сегментация**: выделение и классификация основных единиц текста: слов, предложений, абзацев - **предобработка текста**\n",
    "2. **Морфологический анализ**:\n",
    "    * Выделение ключевых слов\n",
    "    * Нормализация словоформ или их стемминг\n",
    "    * Определение морфологических характеристик\n",
    "3. **Постморфологический анализ**: разрешение морфологической неоднозначности (снятие омонимии)\n",
    "4. **Синтаксический анализ**: построение синтаксической структуры предложения\n",
    "5. **Семантический и дискурсивный анализы**\n",
    "    * Представление семантики (свойства объектов, их отношения, состояния, действия)\n",
    "    * Определение смысла\n",
    "\n",
    "Подходы:\n",
    "* Статистический\n",
    "* Инженерный\n",
    "* ML\n",
    "* Комбинированный"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Графематический анализ (сегментация - разбиение текста на части)\n",
    "* Сегментация нижнего уровня\n",
    "    * Токенизация (токен - минимальный значимый элемент текста: слово, знаки препинания, числа, даты и т.д.)\n",
    "    * Разбиение текста на предложения\n",
    "* Сегментация высокого уровня\n",
    "    * Снтаксическая сегментация\n",
    "    * Макросинтаксический анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kate/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Токенизатор NLTK\n",
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import regexp_tokenize\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Мой дядя самых честных правил,\",\n",
    "    \"Когда не в шутку занемог,\",\n",
    "    \"Он уважать себя заставил\",\n",
    "    \"И лучше выдумать не мог.\"\n",
    "]\n",
    "\n",
    "text = \" \".join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp_tokenizer = (lambda s: regexp_tokenize(s, r\"[\\.\\,\\/\\\\\\?\\!\\@\\#\\:\\~\\`\\\"\\&\\*\\(\\)\\_\\-\\+\\=\\{\\}\\[\\]\\t\\n\\ ]\", gaps=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Мой',\n",
       " 'дядя',\n",
       " 'самых',\n",
       " 'честных',\n",
       " 'правил',\n",
       " ',',\n",
       " 'Когда',\n",
       " 'не',\n",
       " 'в',\n",
       " 'шутку',\n",
       " 'занемог',\n",
       " ',',\n",
       " 'Он',\n",
       " 'уважать',\n",
       " 'себя',\n",
       " 'заставил',\n",
       " 'И',\n",
       " 'лучше',\n",
       " 'выдумать',\n",
       " 'не',\n",
       " 'мог',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Мой',\n",
       " 'дядя',\n",
       " 'самых',\n",
       " 'честных',\n",
       " 'правил',\n",
       " 'Когда',\n",
       " 'не',\n",
       " 'в',\n",
       " 'шутку',\n",
       " 'занемог',\n",
       " 'Он',\n",
       " 'уважать',\n",
       " 'себя',\n",
       " 'заставил',\n",
       " 'И',\n",
       " 'лучше',\n",
       " 'выдумать',\n",
       " 'не',\n",
       " 'мог']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Морфологический анализ\n",
    "\n",
    "Терминология:\n",
    "* Словоформа - конкретная грамматическая форма слова.\n",
    "* Лексема - множество всех словоформ слова, обозначающих одно понятие.\n",
    "* Лемма - нормальная форма лексемы.\n",
    "* Основа (stem) - часть слова без окончания и постфикса (может изменяться).\n",
    "* Псевдооснова - неизменяемая начальная часть слова.\n",
    "\n",
    "Морфологический анализ:\n",
    "* Теггинг - проставление тегов к словоформам.\n",
    "* Стемминг - отсечение окончания слова по правилам.\n",
    "* Лемматизация (нормализация) - преобразование окончания слова по правилам.\n",
    "\n",
    "**Замечание**: стемминг хорошо подходит для английского, а лемматизация - для русского."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SnowballStemmer\n",
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм не использует баз основ слов, а лишь, применяя последовательно ряд правил, отсекает окончания и суффиксы, основываясь на особенностях языка, в связи с чем работает быстро, но не всегда безошибочно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'котики играли в клубочк'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"russian\") \n",
    "stemmer.stem(\"котики играли в клубочки\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mystem — морфологический анализатор русского языка с поддержкой снятия морфологической неоднозначности.\n",
    "Программа работает на основе словаря и способна формировать морфологические гипотезы о незнакомых словах.\n",
    "Словарь представлен в виде набора основ слов и набора всех возможных суффиксов. При этом отличие одного от другого либо известно заранее, либо вычисляется на основании имеющегося словаря."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['котик', ' ', 'играть', ' ', 'в', ' ', 'клубочек', '\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Mystem()\n",
    "m.lemmatize(\"котики играли в клубочки\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Синтаксический анализ\n",
    "\n",
    "Синтаксический анализ - установление связей между словами, а также типов этих связей. Оперирует над предложениями.\n",
    "\n",
    "Подходы:\n",
    "* Деревья составляющих - для языков со строгим порядком слов.\n",
    "* Деревья зависимостей - для языков со свободным порядком слов.\n",
    "* Гибридные модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Синтаксический анализатор - Pattern (библиотека для веб-майнинга)\n",
    "# https://www.clips.uantwerpen.be/pattern\n",
    "# https://github.com/clips/pattern\n",
    "# !pip install pattern\n",
    "from pattern.en import parse, parsetree, pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция parse() принимает строку текста и возвращает строку Unicode с тегами частей речи.\n",
    "\n",
    "Функция parsetree() возвращает объект Text (список объектов предложения).\n",
    "\n",
    "parse или parsetree(\n",
    "* string, \n",
    "* tokenize = True, # Split punctuation marks from words?\n",
    "* tags = True, # Parse part-of-speech tags? (NN, JJ, ...)\n",
    "* chunks = True, # Parse chunks? (NP, VP, PNP, ...)\n",
    "* relations = False, # Parse chunk relations? (-SBJ, -OBJ, ...)\n",
    "* lemmata = False, # Parse lemmata? (ate => eat)\n",
    "* encoding = 'utf-8', # Input string encoding.\n",
    "* tagset = None # Penn Treebank II (default) or UNIVERSAL.\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          WORD   TAG    CHUNK   ROLE   ID     PNP    LEMMA   \n",
      "                                                             \n",
      "             I   PRP    NP      -      -      -      -       \n",
      "           eat   VBP    VP      -      -      -      -       \n",
      "         pizza   NN     NP      -      -      -      -       \n",
      "          with   IN     PP      -      -      PNP    -       \n",
      "             a   DT     NP      -      -      PNP    -       \n",
      "          fork   NN     NP ^    -      -      PNP    -       \n",
      "             .   .      -       -      -      -      -       \n"
     ]
    }
   ],
   "source": [
    "pprint(parse('I eat pizza with a fork.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          WORD   TAG    CHUNK   ROLE   ID     PNP    LEMMA   \n",
      "                                                             \n",
      "             I   PRP    NP      SBJ    1      -      i       \n",
      "           ate   VBD    VP      -      1      -      eat     \n",
      "         pizza   NN     NP      OBJ    1      -      pizza   \n",
      "             .   .      -       -      -      -      .       \n"
     ]
    }
   ],
   "source": [
    "pprint(parse('I ate pizza.', relations=True, lemmata=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['I', 'PRP', 'B-NP', 'O'], ['ate', 'VBD', 'B-VP', 'O'], ['pizza', 'NN', 'B-NP', 'O'], ['.', '.', 'O', 'O']]]\n"
     ]
    }
   ],
   "source": [
    "print(parse ('I ate pizza.').split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          WORD   TAG    CHUNK   ROLE   ID     PNP    LEMMA   \n",
      "                                                             \n",
      "[Sentence('The   DT     NP      -      -      -      -       \n",
      "           cat   NN     NP ^    -      -      -      -       \n",
      "           sat   VBD    VP      -      -      -      -       \n",
      "            on   IN     PP      -      -      PNP    -       \n",
      "           the   DT     NP      -      -      PNP    -       \n",
      "           mat   NN     NP ^    -      -      PNP    -       \n",
      "             .   .      -       -      -      -      -       \n"
     ]
    }
   ],
   "source": [
    "s = parsetree('The cat sat on the mat.', relations=True, lemmata=True)\n",
    "pprint(repr(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP [('The', 'DT'), ('cat', 'NN')]\n",
      "VP [('sat', 'VBD')]\n",
      "PP [('on', 'IN')]\n",
      "NP [('the', 'DT'), ('mat', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for sentence in s:\n",
    "    for chunk in sentence.chunks:\n",
    "        print(chunk.type, [(w.string, w.type) for w in chunk.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Семантический анализ\n",
    "* Локальный семантический анализ (единицы текста):\n",
    "    * Устранение языковых особенностей и культурных контекстов\n",
    "    * Определение семантики слов/словосочетаний\n",
    "    * Установление семантических отношений между словами\n",
    "    * Разрешение многозначности\n",
    "* Глубинный семантический анализ (текст рассматривается целиком и предполагается, что он связан):\n",
    "    * Локальная связанность\n",
    "    * Глобальная связанность (тематическая или дискурсивная)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install nltk\n",
    "# pip3 install spacy\n",
    "# python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here --> advmod --> ADV\n",
      "is --> ROOT --> AUX\n",
      "how --> advmod --> SCONJ\n",
      "you --> nsubj --> PRON\n",
      "can --> aux --> AUX\n",
      "keep --> ccomp --> VERB\n",
      "your --> poss --> PRON\n",
      "car --> dobj --> NOUN\n",
      "and --> cc --> CCONJ\n",
      "other --> amod --> ADJ\n",
      "vehicles --> conj --> NOUN\n",
      "clean --> oprd --> ADJ\n",
      ". --> punct --> PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Here is how you can keep your car and other vehicles clean.\")\n",
    "\n",
    "for tok in doc:\n",
    "    print(tok.text, \"-->\",tok.dep_, \"-->\",tok.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'car and other vehicles'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'DEP':'amod', 'OP':\"?\"},\n",
    "           {'POS':'NOUN'},\n",
    "           {'LOWER': 'and', 'OP':\"?\"},\n",
    "           {'LOWER': 'or', 'OP':\"?\"},\n",
    "           {'LOWER': 'other'},\n",
    "           {'POS': 'NOUN'}]\n",
    "           \n",
    "matcher.add(\"matching_1\", [pattern])\n",
    "\n",
    "matches = matcher(doc)\n",
    "span = doc[matches[0][1]:matches[0][2]]\n",
    "span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The --> det --> DET\n",
      "quick --> amod --> ADJ\n",
      "brown --> amod --> ADJ\n",
      "fox --> nsubj --> NOUN\n",
      "jumps --> ROOT --> VERB\n",
      "over --> prep --> ADP\n",
      "the --> det --> DET\n",
      "lazy --> amod --> ADJ\n",
      "dog --> pobj --> NOUN\n",
      ". --> punct --> PUNCT\n"
     ]
    }
   ],
   "source": [
    "# инициализация spacy\n",
    "doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
    "\n",
    "for tok in doc:\n",
    "    print(tok.text, \"-->\",tok.dep_,\"-->\", tok.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вывод по поиску noun, pronoun, verb, adj:\n",
    "def subtree_matcher(doc):\n",
    "    X = []\n",
    "    Y = []\n",
    "    subjpass = 0\n",
    "    for i, tok in enumerate(doc):\n",
    "        if tok.dep_ == 'nsubjpass':\n",
    "            subjpass = 1\n",
    "            Y.append(tok.text)\n",
    "            break\n",
    "\n",
    "    if subjpass:\n",
    "        for i, tok in enumerate(doc):\n",
    "            if tok.dep_ == \"nsubj\":\n",
    "                Y.append(tok.text)\n",
    "\n",
    "            if tok.dep_ in [\"pobj\", \"dobj\"]:\n",
    "                X.append(tok.text)\n",
    "    else:\n",
    "        for i, tok in enumerate(doc):\n",
    "            if tok.dep_ == \"nsubj\":\n",
    "                X.append(tok.text)\n",
    "\n",
    "            if tok.dep_ in [\"pobj\", \"dobj\"]:\n",
    "                Y.append(tok.text)\n",
    "    \n",
    "    for pair in list(itertools.product(X, Y)):\n",
    "        print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"e3d6930cb45248c3ae3f636f0331c092-0\" class=\"displacy\" width=\"1625\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">quick</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">brown</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">fox</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">jumps</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">over</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">lazy</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">dog..</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e3d6930cb45248c3ae3f636f0331c092-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 575.0,2.0 575.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e3d6930cb45248c3ae3f636f0331c092-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e3d6930cb45248c3ae3f636f0331c092-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e3d6930cb45248c3ae3f636f0331c092-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e3d6930cb45248c3ae3f636f0331c092-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e3d6930cb45248c3ae3f636f0331c092-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e3d6930cb45248c3ae3f636f0331c092-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e3d6930cb45248c3ae3f636f0331c092-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e3d6930cb45248c3ae3f636f0331c092-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e3d6930cb45248c3ae3f636f0331c092-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e3d6930cb45248c3ae3f636f0331c092-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,89.5 1445.0,89.5 1445.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e3d6930cb45248c3ae3f636f0331c092-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e3d6930cb45248c3ae3f636f0331c092-0-6\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e3d6930cb45248c3ae3f636f0331c092-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,266.5 L1287,254.5 1303,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e3d6930cb45248c3ae3f636f0331c092-0-7\" stroke-width=\"2px\" d=\"M945,264.5 C945,2.0 1450.0,2.0 1450.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e3d6930cb45248c3ae3f636f0331c092-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1450.0,266.5 L1458.0,254.5 1442.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog.\n",
      "('fox', 'dog')\n"
     ]
    }
   ],
   "source": [
    "for sent in text.split('. '):\n",
    "    doc = nlp(sent + '.')\n",
    "    displacy.render(doc, style=\"dep\", jupyter=True)\n",
    "    print(sent)\n",
    "    subtree_matcher(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
