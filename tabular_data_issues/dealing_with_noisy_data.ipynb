{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка моделей на шумных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пройтись по ошибкам валидации: интерпретация и визуализация ML-моделей\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"400\" height=\"220\" src=\"pictures/output_10_0.png\"/>\n",
    "</p>\n",
    "\n",
    "### Новые инструменты для интерпретации и визуализации ML-моделей\n",
    "\n",
    "#### Yellowbrick — расширение scikit-learn\n",
    "* [Документация](https://www.scikit-yb.org/en/latest/quickstart.html#using-yellowbrick)\n",
    "* [Machine Learning Visualizations with Yellowbrick](https://medium.com/data-science-community-srm/machine-learning-visualizations-with-yellowbrick-3c533955b1b3)\n",
    "\n",
    "#### ELI5\n",
    "Еще одна визуальная библиотека для устранения ошибок в моделях и объяснения прогнозов (совместима со scikit-learn, XGBoost и Keras).\n",
    "* [Документация](https://eli5.readthedocs.io/en/latest/tutorials/sklearn-text.html)\n",
    "\n",
    "#### MLxtend - визуализация данных, сравнение решений и входящих в состав классификаторов\n",
    "* [Документация](http://rasbt.github.io/mlxtend/)\n",
    "\n",
    "#### LIME – пакет для интерпретации прогнозов\n",
    "* [Документация](https://github.com/marcotcr/lime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Важность фичей и их распределение на тренировочной и тестовой выборке\n",
    "\n",
    "### Population Stability Index\n",
    "\n",
    "PSI - это показатель, использующийся для мониторинга актуальности фичей модели. Он определяет изменения в распределении фичей и может использоваться для их отбора. При помощи PSI мы можем оценить актуальность и репрезентативность нашей выборки.\n",
    "\n",
    "Мы можем назвать фичу нестабильной, если ее распределение меняется в зависимости от каких-то внешних событий. Нестабильные фичи могут внести шум в данные и повлиять на производительность модели.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"800\" height=\"800\" src=\"pictures/psi.png\"/>\n",
    "</p>\n",
    "\n",
    "Вычисление PSI:\n",
    "$$PSI = \\sum_{i=1}^N (y_{ti} - y_{bi}) \\log{(\\frac{y_{ti}}{y_{bi}})}$$\n",
    "где:\n",
    "* $y_{ti}$ - значение показателя целевого периода, попавшая в i-й интервал\n",
    "* $y_{bi}$ - значение показателя базового периода\n",
    "\n",
    "PSI можно интерпретировать следующим образом:\n",
    "* PSI < 10% - отсутствие значимого изменения в текущей выборке;\n",
    "* 10% < PSI < 25% - незначительное изменение;\n",
    "* PSI > 25% - значительное смещение, которое требует перестроения модели.\n",
    "\n",
    "Литература:\n",
    "* [Индекс стабильности популяции (Population stability Index)](https://wiki.loginom.ru/articles/stability-index.html)\n",
    "* [Selecting Features with the Population Stability Index](https://trainindata.medium.com/how-to-select-features-based-on-the-population-stability-index-58a1f0283583)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Переразметить часть данных\n",
    "\n",
    "* Можно попытаться сделать это вручную (долго и дорого)\n",
    "* [Classification with Rejection Based on Cost-sensitive Classification](https://arxiv.org/pdf/2010.11748.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удалить выбросы\n",
    "\n",
    "* [Multivariate outlier detection in Python](https://towardsdatascience.com/multi-variate-outlier-detection-in-python-e900a338da10)\n",
    "* [5 Outlier Detection Techniques that every “Data Enthusiast” Must Know](https://towardsdatascience.com/5-outlier-detection-methods-that-every-data-enthusiast-must-know-f917bf439210)\n",
    "* [Поиск аномалий (Anomaly Detection)](https://dyakonov.org/2017/04/19/%D0%BF%D0%BE%D0%B8%D1%81%D0%BA-%D0%B0%D0%BD%D0%BE%D0%BC%D0%B0%D0%BB%D0%B8%D0%B9-anomaly-detection/comment-page-1/)\n",
    "* [Anomaly Detection via an Isolation Forest](https://deepnote.com/@christopher-hui/Anomaly-Detection-Analysis-Isolation-Forest-c012da68-8081-4e2e-9bc8-8bc59a1c2d6c)\n",
    "* [Anomaly detection using isolation forest](https://www.analyticsvidhya.com/blog/2021/07/anomaly-detection-using-isolation-forest-a-complete-guide/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Статистические критерии: точечные и интервальные оценки\n",
    "\n",
    "Для точечных оценок: непараметрические критерии\n",
    "\n",
    "Для интервальных оценок: бутстрап\n",
    "\n",
    "Литература:\n",
    "* [Statistical Significance Tests for Comparing Machine Learning Algorithms](https://machinelearningmastery.com/statistical-significance-tests-for-comparing-machine-learning-algorithms/)\n",
    "* [Model evaluation, model selection, and algorithm selection in machine learning](https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html)\n",
    "* [Habr: Kaggle Mercedes и кросс-валидация](https://habr.com/ru/company/ods/blog/336168/)\n",
    "* [YouTube: Kaggle Mercedes Benz: предсказание времени тестирования автомобилей](https://www.youtube.com/watch?v=HT3QpRp2ewA&ab_channel=ODSAIRu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling\n",
    "\n",
    "* [SMOTE for Imbalanced Classification with Python](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)\n",
    "* [Radial-Based oversampling for noisy imbalanced data classification](https://www.sciencedirect.com/science/article/abs/pii/S0925231219301596)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley Additive Explanations\n",
    "\n",
    "Рассматривается вклад признаков в итоговое предсказание модели как задача теории игр: игра – процесс, в котором участвуют две и более сторон, ведущих борьбу за свои интересы. Каждая сторона имеет свою цель и использует собственную стратегию, которая может привести к выигрышу или проигрышу, в зависимости от поведения других игроков. \n",
    "\n",
    "Определить оптимальное распределение выигрыша между игроками можно с помощью SAE. Он представляет собой распределение, в котором выигрыш каждого игрока равен его среднему вкладу в общее благосостояние при определенном механизме формирования коалиции.\n",
    "\n",
    "Применяя вышеизложенные положения теории игр к интерпретации ML-моделей, можно сделать следующие выводы:\n",
    "* результат обучения с учителем (на основе заданного примера) – это игра;\n",
    "* выигрыш – это разница между матожиданием результата на всех имеющихся примерах и результатом, полученном на заданном примере;\n",
    "* вклады игроков в игру – влияние каждого значения признака на выигрыш, т.е. результат.\n",
    "\n",
    "При расчёте вектора Шепли необходимо формировать коалиции из ограниченного набора признаков. Однако, не каждая ML-модель позволяет просто убрать признак без повторного обучения модели «с нуля». Потому для формирования коалиций обычно не убирают «лишние» признаки, а заменяют их на случайные значения из «фонового» набора данных. Усреднённый результат модели со случайными значениями признака эквивалентен результату модели, в которой этот признак вообще отсутствует.\n",
    "\n",
    "Литература:\n",
    "* [slundberg/shap](https://github.com/slundberg/shap)\n",
    "* [SHAP (SHapley Additive exPlanations)](https://christophm.github.io/interpretable-ml-book/shap.html)\n",
    "* [Shapley Additive Explanations (SHAP) for Average Attributions](https://ema.drwhy.ai/shapley.html)\n",
    "* [Интерпретируй это: метод SHAP в Data Science](https://chernobrovov.ru/articles/interpretiruj-eto-metod-shap-v-data-science.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модели со случайными предсказаниями\n",
    "\n",
    "#### Регрессия\n",
    "Модели со случайными предсказаниями предсказывают распределение признаков. Они устойчивы к выбросам (за счет того, что на сложных примерах меньше штрафуют модель), могут работать с многомодальными данными (можно предсказывать смеси распределений), хорошо работают с нестандартными данными. Используют кросс-энтропийный китерий.\n",
    "\n",
    "#### Классификация\n",
    "Задача Metric Learning: мы хотим построить отображение входных данных в скрытое пространство таким образом, что полученные эмбединги элементов одного класса были близки друг к другу (например, по l2 ли cos), а эмбединги элементов разных классов были далеки друг от друга. Для этого есть специальные функции потерь, например, Contrastive Loss (by Yann Le Cunn), Triplet loss.\n",
    "* Плохо обучается, когда много выбросов. Решение: перейти к предсказанию распределения векторов.\n",
    "\n",
    "Литература:\n",
    "* [Модели со случайными предсказаниями — Иван Карпухин, Тинькофф](https://www.youtube.com/watch?v=-IbiKlAJqSM&ab_channel=IT%27sTinkoff)\n",
    "* [Metric Learning Tips & Tricks](https://towardsdatascience.com/metric-learning-tips-n-tricks-2e4cfee6b75b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
